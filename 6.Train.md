# ç¬¬å…­ç« ï¼šå¤§æ¨¡å‹è®­ç»ƒæµç¨‹å®è·µï¼ˆå°ç™½å‹å¥½ç‰ˆï¼‰

## 6.1 ä»€ä¹ˆæ˜¯å¤§æ¨¡å‹è®­ç»ƒæµç¨‹ï¼Ÿ

å¤§æ¨¡å‹è®­ç»ƒæµç¨‹å°±åƒ**æ•™ä¸€ä¸ªAIå­¦ç”Ÿå­¦ä¹ çŸ¥è¯†**çš„å…¨è¿‡ç¨‹ï¼š

```
å‡†å¤‡æ•™æ â†’ åŸºç¡€å­¦ä¹  â†’ ä¸“é¡¹è®­ç»ƒ â†’ è€ƒè¯•æ£€æŸ¥ â†’ ä¸Šå²—å·¥ä½œ
   â†“         â†“         â†“         â†“         â†“
 æ•°æ®å‡†å¤‡   é¢„è®­ç»ƒ     SFTå¾®è°ƒ    æ¨¡å‹è¯„ä¼°   éƒ¨ç½²æœåŠ¡
```

### 6.1.1 æ ¸å¿ƒæ¦‚å¿µï¼ˆå¤§ç™½è¯ç‰ˆï¼‰

- **é¢„è®­ç»ƒ**ï¼šè®©AIè¯»å¤§é‡ä¹¦ç±ï¼Œå­¦ä¼šåŸºæœ¬çš„è¯­è¨€èƒ½åŠ›ï¼ˆåƒå°å­¦ç”Ÿå­¦ä¹ ï¼‰
- **SFTå¾®è°ƒ**ï¼šæ•™AIå¦‚ä½•å¯¹è¯ï¼Œå­¦ä¼šå›ç­”é—®é¢˜ï¼ˆåƒå¤§å­¦ç”Ÿä¸“ä¸šè®­ç»ƒï¼‰
- **LoRA**ï¼šä¸€ç§çœé’±çš„æ–¹æ³•ï¼Œä¸ç”¨é‡æ–°è®­ç»ƒæ•´ä¸ªæ¨¡å‹ï¼ˆåƒæˆ´çœ¼é•œçŸ«æ­£è§†åŠ›ï¼‰
- **DeepSpeed**ï¼šè®©è®­ç»ƒæ›´å¿«çš„å·¥å…·ï¼ˆåƒç»™æ±½è½¦åŠ æ¶¡è½®å¢å‹ï¼‰

### 6.1.2 è®­ç»ƒæµç¨‹å›¾

```
å¼€å§‹ â†’ å‡†å¤‡æ•°æ® â†’ é¢„è®­ç»ƒ â†’ SFTè®­ç»ƒ â†’ è¯„ä¼° â†’ éƒ¨ç½² â†’ ç»“æŸ
        â†“         â†“        â†“       â†“       â†“
      æ–‡æœ¬ä¹¦   åŸºç¡€èƒ½åŠ›  å¯¹è¯èƒ½åŠ›  è€ƒè¯•æˆç»©  APIæœåŠ¡
```

## 6.2 ç¯å¢ƒé…ç½®ï¼šå®‰è£…å¿…è¦çš„å·¥å…·

### 6.2.1 éœ€è¦å®‰è£…ä»€ä¹ˆï¼Ÿ

```python
# åƒç»™æ‰‹æœºå®‰è£…APPä¸€æ ·ï¼Œæˆ‘ä»¬éœ€è¦è¿™äº›å·¥å…·ï¼š
# torch - æ·±åº¦å­¦ä¹ çš„"æ“ä½œç³»ç»Ÿ"
# transformers - Hugging Faceçš„"æ¨¡å‹åº“"
# datasets - æ•°æ®å¤„ç†çš„"å·¥å…·ç®±"
# accelerate - GPUåŠ é€Ÿçš„"æ²¹é—¨"
# deepspeed - å¤šGPUè®­ç»ƒçš„"æ‰©éŸ³å™¨"
# peft - èŠ‚çœå†…å­˜çš„"å‹ç¼©åŒ…"
```

### 6.2.2 å®‰è£…å‘½ä»¤

```bash
# ä¸€è¡Œå‘½ä»¤å®‰è£…æ‰€æœ‰å·¥å…·
pip install transformers datasets torch accelerate deepspeed peft

# å¦‚æœæœ‰GPUï¼Œå®‰è£…å¯¹åº”ç‰ˆæœ¬çš„PyTorch
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

## 6.3 æ•°æ®å‡†å¤‡ï¼šç»™AIå‡†å¤‡æ•™æ

### 6.3.1 é¢„è®­ç»ƒæ•°æ®ï¼ˆåŸºç¡€æ•™æï¼‰

```python
# é¢„è®­ç»ƒæ•°æ®å°±åƒå¤§é‡çš„ä¹¦ç±ã€æ–‡ç« 
from datasets import load_dataset

def load_pretrain_data(data_path):
    """åŠ è½½é¢„è®­ç»ƒæ•°æ®"""
    # ä»JSONæ–‡ä»¶åŠ è½½æ•°æ®
    dataset = load_dataset("json", data_files=data_path, split="train")
    
    def clean_text(example):
        # æ¸…ç†æ–‡æœ¬ï¼šå»æ‰å¤šä½™çš„ç©ºæ ¼ã€æ¢è¡Œç­‰
        text = example["text"].strip()
        return {"text": text}
    
    # åº”ç”¨æ¸…ç†å‡½æ•°
    dataset = dataset.map(clean_text)
    return dataset

# ä½¿ç”¨ç¤ºä¾‹
# å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªcorpus.jsonlæ–‡ä»¶ï¼ŒåŒ…å«å¤§é‡æ–‡ç« 
pretrain_dataset = load_pretrain_data("corpus.jsonl")
```

**å°ç™½è§£é‡Š**ï¼š
- è¿™å°±åƒç»™å­¦ç”Ÿå‡†å¤‡å¤§é‡çš„ä¹¦ç±ã€æ–‡ç« è®©ä»–ä»¬é˜…è¯»
- æ•°æ®æ ¼å¼ï¼šæ¯è¡Œä¸€ä¸ªJSONå¯¹è±¡ï¼ŒåŒ…å«"text"å­—æ®µ
- ç¤ºä¾‹æ•°æ®ï¼š`{"text": "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯..."}`

### 6.3.2 SFTæ•°æ®ï¼ˆå¯¹è¯æ•™æï¼‰

```python
def load_sft_data(data_path):
    """åŠ è½½å¯¹è¯æ•°æ®"""
    dataset = load_dataset("json", data_files=data_path, split="train")
    
    def format_conversation(example):
        """æ ¼å¼åŒ–å¯¹è¯æ•°æ®"""
        conversations = example["conversations"]
        
        # æ„å»ºå¯¹è¯æ ¼å¼çš„æ–‡æœ¬
        formatted_text = ""
        for conv in conversations:
            role = conv["role"]  # è§’è‰²ï¼šsystem/user/assistant
            content = conv["content"]  # å†…å®¹
            
            if role == "system":
                formatted_text += f"ç³»ç»Ÿï¼š{content}\n"
            elif role == "user":
                formatted_text += f"ç”¨æˆ·ï¼š{content}\n"
            elif role == "assistant":
                formatted_text += f"åŠ©æ‰‹ï¼š{content}\n"
        
        return {"text": formatted_text}
    
    dataset = dataset.map(format_conversation)
    return dataset

# ä½¿ç”¨ç¤ºä¾‹
sft_dataset = load_sft_data("conversations.jsonl")
```

**å°ç™½è§£é‡Š**ï¼š
- SFTæ•°æ®å°±åƒå¯¹è¯æ•™æï¼Œæ•™AIå¦‚ä½•èŠå¤©
- æ•°æ®æ ¼å¼ï¼šå¤šè½®å¯¹è¯ï¼ŒåŒ…å«ç”¨æˆ·å’ŒåŠ©æ‰‹çš„å¯¹è¯
- ç¤ºä¾‹æ•°æ®ï¼š
```json
{
  "conversations": [
    {"role": "user", "content": "ä½ å¥½"},
    {"role": "assistant", "content": "ä½ å¥½ï¼æˆ‘æ˜¯AIåŠ©æ‰‹"}
  ]
}
```

### 6.3.3 æ•°æ®å¤„ç†ï¼ˆæŠŠæ•™æå˜æˆAIèƒ½çœ‹æ‡‚çš„å½¢å¼ï¼‰

```python
from transformers import AutoTokenizer
import torch

class DataProcessor:
    def __init__(self, tokenizer_path, max_length=512):
        """åˆå§‹åŒ–æ•°æ®å¤„ç†å™¨"""
        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)
        self.max_length = max_length
        
        # è®¾ç½®å¡«å……ç¬¦å·
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
    
    def preprocess_pretrain(self, examples):
        """é¢„å¤„ç†é¢„è®­ç»ƒæ•°æ®"""
        texts = examples["text"]
        
        # åˆ†è¯ï¼šæŠŠæ–‡å­—è½¬æ¢æˆæ•°å­—
        tokenized = self.tokenizer(
            texts,
            truncation=True,  # æˆªæ–­å¤ªé•¿çš„æ–‡æœ¬
            max_length=self.max_length,
            padding="max_length",  # å¡«å……åˆ°å›ºå®šé•¿åº¦
            return_tensors="pt"  # è¿”å›PyTorchå¼ é‡
        )
        
        # åˆ›å»ºæ ‡ç­¾ï¼šé¢„æµ‹ä¸‹ä¸€ä¸ªè¯
        input_ids = tokenized["input_ids"]
        labels = input_ids.clone()
        labels[:, :-1] = input_ids[:, 1:]  # å‘å·¦ç§»åŠ¨ä¸€ä½
        labels[:, -1] = -100  # æœ€åä¸€ä¸ªä½ç½®ä¸è®¡ç®—æŸå¤±
        
        return {
            "input_ids": input_ids,
            "attention_mask": tokenized["attention_mask"],
            "labels": labels
        }
    
    def preprocess_sft(self, examples):
        """é¢„å¤„ç†SFTæ•°æ®"""
        texts = examples["text"]
        
        tokenized = self.tokenizer(
            texts,
            truncation=True,
            max_length=self.max_length,
            padding="max_length",
            return_tensors="pt"
        )
        
        input_ids = tokenized["input_ids"]
        
        # åªè®¡ç®—åŠ©æ‰‹å›å¤çš„æŸå¤±ï¼Œä¸è®¡ç®—ç”¨æˆ·è¾“å…¥çš„æŸå¤±
        loss_mask = self._create_loss_mask(input_ids)
        labels = input_ids.clone()
        labels[loss_mask == 0] = -100
        
        return {
            "input_ids": input_ids,
            "attention_mask": tokenized["attention_mask"],
            "labels": labels
        }
```

**å°ç™½è§£é‡Š**ï¼š
- **Tokenizer**ï¼šæŠŠæ–‡å­—è½¬æ¢æˆæ•°å­—çš„å·¥å…·ï¼ˆåƒå­—å…¸ï¼‰
- **Input_ids**ï¼šæ–‡å­—çš„æ•°å­—è¡¨ç¤º
- **Labels**ï¼šæ ‡å‡†ç­”æ¡ˆï¼ˆä¸‹ä¸€ä¸ªè¯æ˜¯ä»€ä¹ˆï¼‰
- **Attention_mask**ï¼šå‘Šè¯‰AIå“ªäº›éƒ¨åˆ†æ˜¯é‡è¦çš„
- **æŸå¤±æ©ç **ï¼šåªå¯¹AIçš„å›ç­”è®¡ç®—æŸå¤±ï¼Œä¸å¯¹ç”¨æˆ·çš„é—®é¢˜è®¡ç®—

## 6.4 æ¨¡å‹æ„å»ºï¼šé€‰æ‹©å’Œé…ç½®AIå­¦ç”Ÿ

### 6.4.1 ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

def load_model(model_name, device="auto"):
    """åŠ è½½é¢„è®­ç»ƒæ¨¡å‹"""
    # è‡ªåŠ¨é€‰æ‹©è®¾å¤‡
    if device == "auto":
        device = "cuda" if torch.cuda.is_available() else "cpu"
    
    print(f"ä½¿ç”¨è®¾å¤‡ï¼š{device}")
    
    # åŠ è½½æ¨¡å‹
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.bfloat16 if device == "cuda" else torch.float32,
        device_map="auto" if device == "cuda" else None
    )
    
    # åŠ è½½åˆ†è¯å™¨
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    
    # è®¾ç½®å¡«å……ç¬¦å·
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    return model, tokenizer

# ä½¿ç”¨ç¤ºä¾‹
model, tokenizer = load_model("gpt2")  # åŠ è½½GPT-2æ¨¡å‹
```

**å°ç™½è§£é‡Š**ï¼š
- **é¢„è®­ç»ƒæ¨¡å‹**ï¼šå·²ç»æœ‰åŸºç¡€çš„AIå­¦ç”Ÿï¼Œåƒå·²ç»è¯»è¿‡å¾ˆå¤šä¹¦çš„å­¦ç”Ÿ
- **GPT-2**ï¼šä¸€ä¸ªå¼€æºçš„è¯­è¨€æ¨¡å‹ï¼Œæœ‰ä¸åŒå¤§å°ï¼ˆsmall, medium, largeï¼‰
- **Device**ï¼šè®­ç»ƒè®¾å¤‡ï¼ˆGPUå¿«ä½†è´µï¼ŒCPUæ…¢ä½†ä¾¿å®œï¼‰
- **Tokenizer**ï¼šæ–‡å­—å’Œæ•°å­—ä¹‹é—´çš„è½¬æ¢å™¨

### 6.4.2 LoRAé…ç½®ï¼ˆçœé’±å¤§æ³•ï¼‰

```python
from peft import LoraConfig, get_peft_model, TaskType

def setup_lora(model, lora_rank=8):
    """é…ç½®LoRAå‚æ•°"""
    lora_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        r=lora_rank,  # LoRAçŸ©é˜µçš„ç§©ï¼ˆé€šå¸¸8-32ï¼‰
        lora_alpha=16,  # ç¼©æ”¾å› å­ï¼ˆé€šå¸¸æ˜¯rankçš„2å€ï¼‰
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],  # åº”ç”¨LoRAçš„å±‚
        lora_dropout=0.1,  # Dropoutç‡
        bias="none"  # ä¸è®­ç»ƒåç½®
    )
    
    # åº”ç”¨LoRA
    model = get_peft_model(model, lora_config)
    
    # æ‰“å°å¯è®­ç»ƒå‚æ•°
    print("LoRAé…ç½®å®Œæˆï¼Œå¯è®­ç»ƒå‚æ•°ï¼š")
    model.print_trainable_parameters()
    
    return model

# ä½¿ç”¨ç¤ºä¾‹
model = setup_lora(model, lora_rank=8)
```

**å°ç™½è§£é‡Š**ï¼š
- **LoRA**ï¼šä¸€ç§å‚æ•°é«˜æ•ˆçš„å¾®è°ƒæ–¹æ³•ï¼Œåªè®­ç»ƒä¸€å°éƒ¨åˆ†å‚æ•°
- **Rank**ï¼šLoRAçŸ©é˜µçš„å¤§å°ï¼Œè¶Šå¤§æ•ˆæœè¶Šå¥½ä½†å‚æ•°è¶Šå¤š
- **Target_modules**ï¼šåº”ç”¨LoRAçš„æ¨¡å‹å±‚
- **ä¼˜åŠ¿**ï¼šè®­ç»ƒé€Ÿåº¦å¿«ï¼Œå†…å­˜å ç”¨å°ï¼Œæ•ˆæœä¹Ÿä¸é”™

## 6.5 è®­ç»ƒé…ç½®ï¼šè®¾ç½®å­¦ä¹ è®¡åˆ’

### 6.5.1 é¢„è®­ç»ƒé…ç½®

```python
from transformers import TrainingArguments

def setup_pretrain_args(output_dir, learning_rate=2e-4, batch_size=8, num_epochs=3):
    """é…ç½®é¢„è®­ç»ƒå‚æ•°"""
    training_args = TrainingArguments(
        output_dir=output_dir,  # è¾“å‡ºç›®å½•
        num_train_epochs=num_epochs,  # è®­ç»ƒè½®æ•°
        per_device_train_batch_size=batch_size,  # æ¯ä¸ªè®¾å¤‡çš„æ‰¹æ¬¡å¤§å°
        gradient_accumulation_steps=4,  # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
        learning_rate=learning_rate,  # å­¦ä¹ ç‡
        weight_decay=0.01,  # æƒé‡è¡°å‡
        warmup_steps=1000,  # é¢„çƒ­æ­¥æ•°
        lr_scheduler_type="cosine",  # å­¦ä¹ ç‡è°ƒåº¦å™¨
        logging_steps=100,  # æ—¥å¿—æ­¥æ•°
        save_steps=1000,  # ä¿å­˜æ­¥æ•°
        save_total_limit=3,  # ä¿å­˜çš„æ¨¡å‹æ•°é‡é™åˆ¶
        fp16=True,  # æ··åˆç²¾åº¦è®­ç»ƒ
        gradient_checkpointing=True,  # æ¢¯åº¦æ£€æŸ¥ç‚¹
        dataloader_num_workers=4,  # æ•°æ®åŠ è½½å™¨å·¥ä½œçº¿ç¨‹æ•°
        report_to="wandb",  # å®éªŒè·Ÿè¸ª
        run_name="pretrain_experiment"  # å®éªŒåç§°
    )
    
    return training_args
```

**å°ç™½è§£é‡Š**ï¼š
- **Batch_size**ï¼šä¸€æ¬¡å¤„ç†å¤šå°‘æ•°æ®
- **Learning_rate**ï¼šå­¦ä¹ é€Ÿåº¦ï¼Œå¤ªå¤§å¯èƒ½å­¦ä¸å¥½ï¼Œå¤ªå°å­¦ä¹ æ…¢
- **Epochs**ï¼šæŠŠæ‰€æœ‰æ•°æ®çœ‹å‡ é
- **Gradient_accumulation**ï¼šæ¨¡æ‹Ÿæ›´å¤§çš„batch_size
- **Warmup**ï¼šå…ˆæ…¢åå¿«çš„å­¦ä¹ ç­–ç•¥

### 6.5.2 SFTé…ç½®

```python
def setup_sft_args(output_dir, learning_rate=1e-5, batch_size=4, num_epochs=3):
    """é…ç½®SFTè®­ç»ƒå‚æ•°"""
    training_args = TrainingArguments(
        output_dir=output_dir,
        num_train_epochs=num_epochs,
        per_device_train_batch_size=batch_size,
        gradient_accumulation_steps=8,
        learning_rate=learning_rate,  # SFTé€šå¸¸ä½¿ç”¨è¾ƒå°çš„å­¦ä¹ ç‡
        weight_decay=0.01,
        warmup_ratio=0.03,  # é¢„çƒ­æ¯”ä¾‹
        lr_scheduler_type="cosine",
        logging_steps=50,
        save_steps=500,
        save_total_limit=2,
        fp16=True,
        gradient_checkpointing=True,
        dataloader_num_workers=4,
        report_to="wandb",
        run_name="sft_experiment",
        evaluation_strategy="steps",  # è¯„ä¼°ç­–ç•¥
        eval_steps=500,  # è¯„ä¼°æ­¥æ•°
        load_best_model_at_end=True  # åŠ è½½æœ€ä½³æ¨¡å‹
    )
    
    return training_args
```

**å°ç™½è§£é‡Š**ï¼š
- SFTçš„å­¦ä¹ ç‡é€šå¸¸æ¯”é¢„è®­ç»ƒå°ï¼ˆ1e-5 vs 2e-4ï¼‰
- SFTé€šå¸¸éœ€è¦è¯„ä¼°æ¥é€‰æ‹©æœ€ä½³æ¨¡å‹
- SFTçš„è®­ç»ƒè½®æ•°å¯èƒ½éœ€è¦æ›´å¤š

## 6.6 å¼€å§‹è®­ç»ƒï¼šè®©AIå­¦ç”Ÿå­¦ä¹ 

### 6.6.1 è®­ç»ƒæ‰§è¡Œ

```python
from transformers import Trainer, DataCollatorForLanguageModeling

def train_model(model, tokenizer, dataset, training_args, mode="pretrain"):
    """æ‰§è¡Œè®­ç»ƒ"""
    
    # åˆ›å»ºæ•°æ®å¤„ç†å™¨
    data_processor = DataProcessor(tokenizer_path=tokenizer.name_or_path)
    
    # é¢„å¤„ç†æ•°æ®
    if mode == "pretrain":
        processed_dataset = dataset.map(
            data_processor.preprocess_pretrain,
            batched=True,
            remove_columns=dataset.column_names
        )
    else:  # sft
        processed_dataset = dataset.map(
            data_processor.preprocess_sft,
            batched=True,
            remove_columns=dataset.column_names
        )
    
    # åˆ›å»ºæ•°æ®æ•´ç†å™¨
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False  # ä¸æ˜¯æ©ç è¯­è¨€å»ºæ¨¡
    )
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = Trainer(
        model=model,
        args=training_args,
        data_collator=data_collator,
        train_dataset=processed_dataset,
    )
    
    # å¼€å§‹è®­ç»ƒ
    print(f"å¼€å§‹{mode}è®­ç»ƒ...")
    trainer.train()
    
    # ä¿å­˜æ¨¡å‹
    trainer.save_model()
    tokenizer.save_pretrained(training_args.output_dir)
    
    print(f"è®­ç»ƒå®Œæˆï¼Œæ¨¡å‹ä¿å­˜åœ¨ï¼š{training_args.output_dir}")
    return trainer
```

**å°ç™½è§£é‡Š**ï¼š
- **Trainer**ï¼šHugging Faceæä¾›çš„è®­ç»ƒå™¨ï¼Œç®€åŒ–è®­ç»ƒè¿‡ç¨‹
- **Data_collator**ï¼šæ•°æ®æ•´ç†å™¨ï¼ŒæŠŠæ•°æ®æ•´ç†æˆæ¨¡å‹éœ€è¦çš„æ ¼å¼
- **è®­ç»ƒè¿‡ç¨‹**ï¼šå‰å‘ä¼ æ’­ â†’ è®¡ç®—æŸå¤± â†’ åå‘ä¼ æ’­ â†’ æ›´æ–°å‚æ•°

### 6.6.2 å®Œæ•´è®­ç»ƒæµç¨‹

```python
def full_training_pipeline():
    """å®Œæ•´çš„è®­ç»ƒæµç¨‹"""
    
    # 1. å‡†å¤‡æ•°æ®
    print("å‡†å¤‡æ•°æ®...")
    pretrain_dataset = load_pretrain_data("corpus.jsonl")
    sft_dataset = load_sft_data("conversations.jsonl")
    
    # 2. åŠ è½½æ¨¡å‹
    print("åŠ è½½æ¨¡å‹...")
    model, tokenizer = load_model("gpt2-medium")
    
    # 3. é¢„è®­ç»ƒ
    print("å¼€å§‹é¢„è®­ç»ƒ...")
    pretrain_args = setup_pretrain_args(
        output_dir="./pretrain_output",
        learning_rate=2e-4,
        batch_size=4,
        num_epochs=1  # ç¤ºä¾‹åªç”¨1è½®
    )
    
    pretrain_trainer = train_model(
        model=model,
        tokenizer=tokenizer,
        dataset=pretrain_dataset,
        training_args=pretrain_args,
        mode="pretrain"
    )
    
    # 4. SFTå¾®è°ƒ
    print("å¼€å§‹SFTå¾®è°ƒ...")
    model = setup_lora(model, lora_rank=8)  # é…ç½®LoRA
    
    sft_args = setup_sft_args(
        output_dir="./sft_output",
        learning_rate=1e-5,
        batch_size=2,
        num_epochs=2
    )
    
    sft_trainer = train_model(
        model=model,
        tokenizer=tokenizer,
        dataset=sft_dataset,
        training_args=sft_args,
        mode="sft"
    )
    
    print("è®­ç»ƒæµç¨‹å®Œæˆï¼")
    return pretrain_trainer, sft_trainer
```

## 6.7 æ¨¡å‹è¯„ä¼°ï¼šè€ƒè¯•æ£€æŸ¥

### 6.7.1 å›°æƒ‘åº¦è¯„ä¼°

```python
import math
import torch
from tqdm import tqdm

def evaluate_perplexity(model, tokenizer, dataset, device="cuda"):
    """è¯„ä¼°æ¨¡å‹å›°æƒ‘åº¦"""
    model.eval()
    total_loss = 0
    total_tokens = 0
    
    data_processor = DataProcessor(tokenizer_path=tokenizer.name_or_path)
    
    with torch.no_grad():
        for batch in tqdm(dataset, desc="è¯„ä¼°ä¸­"):
            # é¢„å¤„ç†æ•°æ®
            processed = data_processor.preprocess_pretrain({"text": [batch["text"]]})
            
            input_ids = processed["input_ids"].to(device)
            attention_mask = processed["attention_mask"].to(device)
            labels = processed["labels"].to(device)
            
            # å‰å‘ä¼ æ’­
            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                labels=labels
            )
            
            loss = outputs.loss
            num_tokens = (labels != -100).sum().item()
            
            total_loss += loss.item() * num_tokens
            total_tokens += num_tokens
    
    avg_loss = total_loss / total_tokens
    perplexity = math.exp(avg_loss)
    
    return perplexity
```

**å°ç™½è§£é‡Š**ï¼š
- **å›°æƒ‘åº¦**ï¼šè¡¡é‡æ¨¡å‹é¢„æµ‹èƒ½åŠ›çš„æŒ‡æ ‡ï¼Œè¶Šä½è¶Šå¥½
- **è¯„ä¼°è¿‡ç¨‹**ï¼šè®©æ¨¡å‹é¢„æµ‹æ–‡æœ¬ï¼Œçœ‹é¢„æµ‹å¾—å‡†ä¸å‡†
- **Perplexity < 50**ï¼šä¸é”™ï¼›**Perplexity < 20**ï¼šå¾ˆå¥½

### 6.7.2 ç”Ÿæˆè´¨é‡è¯„ä¼°

```python
def test_generation(model, tokenizer, prompts, max_length=100):
    """æµ‹è¯•ç”Ÿæˆè´¨é‡"""
    model.eval()
    results = []
    
    for prompt in prompts:
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        
        # ç”Ÿæˆæ–‡æœ¬
        outputs = model.generate(
            **inputs,
            max_length=max_length,
            num_return_sequences=1,
            temperature=0.7,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
        
        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        results.append({
            "prompt": prompt,
            "generated": generated_text
        })
    
    return results

# æµ‹è¯•ç¤ºä¾‹
test_prompts = [
    "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚",
    "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
    "å¦‚ä½•å­¦ä¹ æœºå™¨å­¦ä¹ ï¼Ÿ"
]

results = test_generation(model, tokenizer, test_prompts)
for result in results:
    print(f"é—®é¢˜ï¼š{result['prompt']}")
    print(f"å›ç­”ï¼š{result['generated']}\n")
```

## 6.8 éƒ¨ç½²ä½¿ç”¨ï¼šè®©AIä¸Šå²—å·¥ä½œ

### 6.8.1 ç®€å•éƒ¨ç½²

```python
from transformers import pipeline

def deploy_model(model_path):
    """éƒ¨ç½²æ¨¡å‹ä¸ºpipeline"""
    generator = pipeline(
        "text-generation",
        model=model_path,
        tokenizer=model_path,
        device=0 if torch.cuda.is_available() else -1,
        torch_dtype=torch.bfloat16
    )
    
    return generator

# ä½¿ç”¨ç¤ºä¾‹
generator = deploy_model("./sft_output")

# å¯¹è¯æµ‹è¯•
while True:
    user_input = input("ç”¨æˆ·ï¼š")
    if user_input.lower() in ["exit", "quit", "é€€å‡º"]:
        break
    
    result = generator(user_input, max_length=100, num_return_sequences=1)
    print(f"åŠ©æ‰‹ï¼š{result[0]['generated_text']}\n")
```

### 6.8.2 Web APIéƒ¨ç½²

```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import uvicorn

app = FastAPI()

class ChatRequest(BaseModel):
    message: str
    max_length: int = 100
    temperature: float = 0.7

class ChatResponse(BaseModel):
    response: str

# åŠ è½½æ¨¡å‹
generator = deploy_model("./sft_output")

@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """èŠå¤©æ¥å£"""
    try:
        result = generator(
            request.message,
            max_length=request.max_length,
            temperature=request.temperature,
            num_return_sequences=1
        )
        
        return ChatResponse(
            response=result[0]['generated_text']
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    # å¯åŠ¨APIæœåŠ¡
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

## 6.9 å¸¸è§é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ

### 6.9.1 è®­ç»ƒé—®é¢˜

**Q1: GPUå†…å­˜ä¸è¶³æ€ä¹ˆåŠï¼Ÿ**
```python
# è§£å†³æ–¹æ¡ˆï¼š
# 1. å‡å°batch_size
batch_size = 2  # ä»8å‡å°åˆ°2

# 2. ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯
gradient_accumulation_steps = 16  # ç›¸å½“äºbatch_size Ã— 16

# 3. å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
gradient_checkpointing = True

# 4. ä½¿ç”¨LoRA
model = setup_lora(model, lora_rank=8)

# 5. ä½¿ç”¨æ··åˆç²¾åº¦
fp16 = True
```

**Q2: è®­ç»ƒlossä¸ä¸‹é™ï¼Ÿ**
```python
# è§£å†³æ–¹æ¡ˆï¼š
# 1. æ£€æŸ¥å­¦ä¹ ç‡
learning_rate = 2e-4  # é¢„è®­ç»ƒ
learning_rate = 1e-5  # SFT

# 2. æ£€æŸ¥æ•°æ®è´¨é‡
# ç¡®ä¿æ•°æ®æ ¼å¼æ­£ç¡®ï¼Œæ²¡æœ‰ç©ºæ•°æ®

# 3. æ£€æŸ¥æ¨¡å‹åˆå§‹åŒ–
# ç¡®ä¿æ­£ç¡®åŠ è½½äº†é¢„è®­ç»ƒæ¨¡å‹

# 4. å°è¯•ä¸åŒçš„ä¼˜åŒ–å™¨
# from transformers import AdamW
# optimizer = AdamW(model.parameters(), lr=learning_rate)
```

**Q3: ç”Ÿæˆè´¨é‡ä¸å¥½ï¼Ÿ**
```python
# è§£å†³æ–¹æ¡ˆï¼š
# 1. å¢åŠ è®­ç»ƒæ•°æ®
# æ”¶é›†æ›´å¤šé«˜è´¨é‡çš„å¯¹è¯æ•°æ®

# 2. è°ƒæ•´ç”Ÿæˆå‚æ•°
temperature = 0.7  # 0.1-1.0ä¹‹é—´è°ƒæ•´
top_p = 0.9  # 0.7-0.95ä¹‹é—´è°ƒæ•´

# 3. å¢åŠ è®­ç»ƒè½®æ•°
num_epochs = 5  # ä»3å¢åŠ åˆ°5

# 4. ä½¿ç”¨æ›´å¤§çš„æ¨¡å‹
model_name = "gpt2-large"  # ä»gpt2-mediumå‡çº§
```

## 6.10 å®ç”¨æŠ€å·§æ€»ç»“

### 6.10.1 èŠ‚çœå†…å­˜çš„æŠ€å·§

```python
# 1. æ¢¯åº¦æ£€æŸ¥ç‚¹
model.gradient_checkpointing_enable()

# 2. æ··åˆç²¾åº¦è®­ç»ƒ
fp16 = True

# 3. 8ä½ä¼˜åŒ–å™¨
from transformers import AdamW8bit
optimizer = AdamW8bit(model.parameters(), lr=learning_rate)

# 4. æ¢¯åº¦ç´¯ç§¯
gradient_accumulation_steps = 8
```

### 6.10.2 è®­ç»ƒåŠ é€Ÿçš„æŠ€å·§

```python
# 1. ä½¿ç”¨Flash Attention
# éœ€è¦PyTorch 2.0+
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    use_flash_attention_2=True
)

# 2. å¤šGPUè®­ç»ƒ
# ä½¿ç”¨ accelerate
from accelerate import Accelerator
accelerator = Accelerator()

# 3. æ•°æ®å¹¶è¡Œ
import torch.distributed as dist
# ä½¿ç”¨ torchrun å¯åŠ¨
```

### 6.10.3 ç›‘æ§å®éªŒ

```python
import wandb

# åˆå§‹åŒ–wandb
wandb.init(
    project="llm-training",
    name="experiment-1",
    config={
        "learning_rate": 2e-4,
        "batch_size": 4,
        "epochs": 3
    }
)

# è®­ç»ƒè¿‡ç¨‹ä¸­è®°å½•
wandb.log({
    "train_loss": loss.item(),
    "learning_rate": lr,
    "perplexity": perplexity
})
```

## 6.11 å®Œæ•´ç¤ºä¾‹ï¼šç«¯åˆ°ç«¯è®­ç»ƒ

### 6.11.1 å¿«é€Ÿå¼€å§‹è„šæœ¬

```python
# quick_start.py
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer
from datasets import load_dataset
from peft import LoraConfig, get_peft_model, TaskType

def quick_start():
    """å¿«é€Ÿå¼€å§‹è®­ç»ƒ"""
    
    # 1. å‡†å¤‡æ•°æ®
    print("å‡†å¤‡æ•°æ®...")
    dataset = load_dataset("wikitext", "wikitext-2-raw-v1", split="train")
    
    # 2. åŠ è½½æ¨¡å‹å’Œtokenizer
    print("åŠ è½½æ¨¡å‹...")
    model_name = "gpt2"
    model = AutoModelForCausalLM.from_pretrained(model_name)
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # 3. é…ç½®LoRA
    print("é…ç½®LoRA...")
    lora_config = LoraConfig(
        r=8,
        lora_alpha=16,
        target_modules=["c_attn", "c_proj"],
        lora_dropout=0.1,
        bias="none",
        task_type=TaskType.CAUSAL_LM
    )
    model = get_peft_model(model, lora_config)
    
    # 4. æ•°æ®é¢„å¤„ç†
    def tokenize_function(examples):
        return tokenizer(examples["text"], truncation=True, max_length=512, padding="max_length")
    
    tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=dataset.column_names)
    
    # 5. é…ç½®è®­ç»ƒå‚æ•°
    training_args = TrainingArguments(
        output_dir="./quick_output",
        num_train_epochs=1,
        per_device_train_batch_size=2,
        gradient_accumulation_steps=4,
        learning_rate=2e-4,
        fp16=True,
        logging_steps=10,
        save_steps=100,
    )
    
    # 6. å¼€å§‹è®­ç»ƒ
    print("å¼€å§‹è®­ç»ƒ...")
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset,
    )
    trainer.train()
    
    # 7. æµ‹è¯•ç”Ÿæˆ
    print("æµ‹è¯•ç”Ÿæˆ...")
    model.eval()
    prompt = "The future of AI is"
    inputs = tokenizer(prompt, return_tensors="pt")
    
    outputs = model.generate(
        **inputs,
        max_length=50,
        num_return_sequences=1,
        temperature=0.7,
        do_sample=True
    )
    
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    print(f"ç”Ÿæˆç»“æœï¼š{generated_text}")
    
    print("è®­ç»ƒå®Œæˆï¼")

if __name__ == "__main__":
    quick_start()
```

### 6.11.2 ä½¿ç”¨è¯´æ˜

```bash
# 1. å®‰è£…ä¾èµ–
pip install transformers datasets torch accelerate peft

# 2. è¿è¡Œå¿«é€Ÿå¼€å§‹è„šæœ¬
python quick_start.py

# 3. è§‚å¯Ÿè®­ç»ƒè¿‡ç¨‹
# è®­ç»ƒå®Œæˆåä¼šç”Ÿæˆä¸€äº›æ–‡æœ¬
```

## 6.12 æ€»ç»“

### 6.12.1 æœ¬ç« è¦ç‚¹

1. **è®­ç»ƒæµç¨‹**ï¼šæ•°æ®å‡†å¤‡ â†’ é¢„è®­ç»ƒ â†’ SFTå¾®è°ƒ â†’ è¯„ä¼° â†’ éƒ¨ç½²
2. **æ ¸å¿ƒæ¦‚å¿µ**ï¼šé¢„è®­ç»ƒã€SFTã€LoRAã€DeepSpeed
3. **å®ç”¨æŠ€å·§**ï¼šå†…å­˜ä¼˜åŒ–ã€è®­ç»ƒåŠ é€Ÿã€å®éªŒç›‘æ§
4. **å¸¸è§é—®é¢˜**ï¼šå†…å­˜ä¸è¶³ã€lossä¸ä¸‹é™ã€ç”Ÿæˆè´¨é‡å·®

### 6.12.2 å°ç™½å»ºè®®

1. **ä»å°å¼€å§‹**ï¼šå…ˆç”¨å°æ•°æ®é›†å’Œå°æ¨¡å‹æµ‹è¯•
2. **é€æ­¥å¢åŠ **ï¼šç¡®è®¤æµç¨‹æ­£ç¡®åå†æ‰©å¤§è§„æ¨¡
3. **å¤šå®éªŒ**ï¼šå°è¯•ä¸åŒçš„å‚æ•°è®¾ç½®
4. **ä¿æŒè€å¿ƒ**ï¼šå¤§æ¨¡å‹è®­ç»ƒéœ€è¦æ—¶é—´
5. **å–„ç”¨å·¥å…·**ï¼šHugging Faceå·¥å…·è®©è®­ç»ƒæ›´ç®€å•

### 6.12.3 ä¸‹ä¸€æ­¥å­¦ä¹ 

1. **æ·±å…¥ç†è®º**ï¼šå­¦ä¹ Transformeræ¶æ„åŸç†
2. **å®è·µé¡¹ç›®**ï¼šåœ¨è‡ªå·±çš„æ•°æ®é›†ä¸Šè®­ç»ƒ
3. **é«˜çº§æŠ€æœ¯**ï¼šQLoRAã€P-Tuningã€Prefix-Tuning
4. **éƒ¨ç½²ä¼˜åŒ–**ï¼šæ¨¡å‹å‹ç¼©ã€é‡åŒ–ã€æœåŠ¡ä¼˜åŒ–

é€šè¿‡æœ¬ç« çš„å­¦ä¹ ï¼Œä½ å·²ç»æŒæ¡äº†å¤§æ¨¡å‹è®­ç»ƒçš„å®Œæ•´æµç¨‹ã€‚è®°ä½ï¼Œè®­ç»ƒå¤§æ¨¡å‹æ˜¯ä¸€ä¸ªè¿­ä»£çš„è¿‡ç¨‹ï¼Œéœ€è¦ä¸æ–­å®éªŒå’Œä¼˜åŒ–ã€‚ç¥ä½ è®­ç»ƒé¡ºåˆ©ï¼ğŸš€