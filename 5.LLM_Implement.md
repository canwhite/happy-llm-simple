# 第五章：动手搭建大模型（修正版）

## 5.1 实现流程概述

本章将带你从零开始实现一个基于 LLaMA2 架构的大语言模型。整个实现过程分为以下步骤：

1. **训练分词器** - 第一步，也是最基础的一步
2. **准备数据集** - 包括预训练数据和 SFT 数据
3. **构建模型** - 实现 LLaMA2 的核心组件
4. **预训练** - 在大规模语料上训练模型
5. **SFT 微调** - 让模型学会对话
6. **推理生成** - 使用训练好的模型生成文本

## 5.2 训练分词器

### 5.2.1 为什么先训练分词器？

分词器是语言模型的基础，它负责将文本转换为模型能理解的数字序列。一个好的分词器应该：

- 能高效处理大量文本
- 词汇表大小适中（太大则模型参数多，太小则信息损失大）
- 支持特殊 token（如对话开始/结束标记）

### 5.2.2 BPE 算法介绍

我们选择 BPE（Byte Pair Encoding）算法来训练分词器。BPE 的优点：

- 自动发现常见词和子词
- 能处理未登录词（OOV 问题）
- 压缩率高，词汇表大小可控

**核心思想**：不断合并最频繁出现的字符对，直到达到词汇表大小限制。

### 5.2.3 实现代码

```python
# code/train_tokenizer.py
import argparse
import os
import random
import json
from typing import Generator
from transformers import AutoTokenizer
from tokenizers import (
    decoders,
    models,
    pre_tokenizers,
    trainers,
    Tokenizer,
)
from tokenizers.normalizers import NFKC

def train_tokenizer(data_path: str, save_dir: str, vocab_size: int = 6144) -> None:
    """训练并保存自定义tokenizer"""
    os.makedirs(save_dir, exist_ok=True)

    # 初始化BPE分词器
    tokenizer = Tokenizer(models.BPE(unk_token="<unk>"))
    tokenizer.normalizer = NFKC()  # 文本规范化
    tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)
    tokenizer.decoder = decoders.ByteLevel()

    # 定义特殊token
    special_tokens = [
        "<unk>",     # 未知词
        "<s>",       # 句子开始
        "</s>",      # 句子结束
        "<|im_start|>",  # 对话开始
        "<|im_end|>"     # 对话结束
    ]

    # 配置训练器
    trainer = trainers.BpeTrainer(
        vocab_size=vocab_size,
        special_tokens=special_tokens,
        min_frequency=2,  # 至少出现2次才纳入词汇表
        show_progress=True,
        initial_alphabet=pre_tokenizers.ByteLevel.alphabet()
    )

    # 开始训练
    print(f"Training tokenizer with data from {data_path}")
    texts = read_texts_from_jsonl(data_path)
    tokenizer.train_from_iterator(texts, trainer=trainer, length=os.path.getsize(data_path))

    # 验证特殊token映射
    assert tokenizer.token_to_id("<unk>") == 0
    assert tokenizer.token_to_id("<s>") == 1
    assert tokenizer.token_to_id("</s>") == 2
    assert tokenizer.token_to_id("<|im_start|>") == 3
    assert tokenizer.token_to_id("<|im_end|>") == 4

    # 保存分词器
    tokenizer.save(os.path.join(save_dir, "tokenizer.json"))
    create_tokenizer_config(save_dir)
    print(f"Tokenizer saved to {save_dir}")
```

**小白解释**：

1. **初始化**：创建一个 BPE 分词器，设置未知词标记
2. **文本预处理**：NFKC 规范化文本，ByteLevel 预处理
3. **特殊 token**：定义对话所需的特殊标记
4. **训练**：使用 BPE 算法从文本中学习词汇表
5. **验证**：确保特殊 token 的 ID 映射正确

### 5.2.4 使用训练好的分词器

```bash
# 训练分词器
python code/train_tokenizer.py --data_path your_data.jsonl --save_dir tokenizer_k --vocab_size 6144
```

## 5.3 数据集准备

### 5.3.1 预训练数据集

预训练需要大量无标签的文本数据，我们使用出门问问序列猴子数据集。

```python
# code/dataset.py - PretrainDataset类
class PretrainDataset(Dataset):
    def __init__(self, data_path, tokenizer, max_length=512):
        super().__init__()
        self.data_path = data_path
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.padding = tokenizer.pad_token_id if tokenizer.pad_token_id else 0

        with open(data_path, 'r', encoding='utf-8') as f:
            self.data = f.readlines()

    def __getitem__(self, index: int):
        # 读取文本
        sample = json.loads(self.data[index])
        text = f"{self.tokenizer.bos_token if self.tokenizer.bos_token else ''}{sample['text']}"

        # 分词和截断
        input_id = self.tokenizer(text).data['input_ids'][:self.max_length]
        text_len = len(input_id)

        # 填充
        padding_len = self.max_length - text_len
        input_id = input_id + [self.padding] * padding_len

        # 生成损失掩码（1=计算损失，0=忽略）
        loss_mask = [1] * text_len + [0] * padding_len

        # 转换为PyTorch张量
        input_id = np.array(input_id)
        X = np.array(input_id[:-1]).astype(np.int64)  # 输入：去掉最后一个token
        Y = np.array(input_id[1:]).astype(np.int64)  # 标签：去掉第一个token
        loss_mask = np.array(loss_mask[1:]).astype(np.int64)  # 对应的损失掩码

        return torch.from_numpy(X), torch.from_numpy(Y), torch.from_numpy(loss_mask)
```

**小白解释**：

- **输入 X**：序列的前 n-1 个 token
- **标签 Y**：序列的后 n-1 个 token
- **目标**：让模型学会预测下一个词
- **填充**：将短序列填充到固定长度
- **损失掩码**：只对真实文本计算损失，忽略填充部分

### 5.3.2 SFT 数据集

SFT（Supervised Fine-Tuning）数据集是有标签的对话数据。

```python
# code/dataset.py - SFTDataset类
class SFTDataset(Dataset):
    def __getitem__(self, index: int):
        sample = json.loads(self.data[index])

        # 应用对话模板
        text = self.tokenizer.apply_chat_template(sample, tokenize=False, add_generation_prompt=False)
        input_id = self.tokenizer(text).data['input_ids'][:self.max_length]

        # 生成损失掩码 - 只计算AI回答部分的损失
        loss_mask = self.generate_loss_mask(input_id)

        # 与预训练类似的处理...
        return torch.from_numpy(X), torch.from_numpy(Y), torch.from_numpy(loss_mask)

    def generate_loss_mask(self, input_ids):
        """生成损失掩码：只计算assistant回复的损失"""
        mask = [0] * len(input_ids)

        # 查找assistant回复部分
        assistant_start_id = self.tokenizer.convert_tokens_to_ids("<|im_start|>")
        assistant_end_id = self.tokenizer.convert_tokens_to_ids("<|im_end|>")

        i = 0
        n = len(input_ids)

        while i < n:
            # 找到 <|im_start|>assistant
            if input_ids[i] == assistant_start_id and i + 2 < n and input_ids[i+2] == self.tokenizer.convert_tokens_to_ids("assistant"):
                # 跳到内容开始
                j = i + 3
                if j < n and input_ids[j] == self.tokenizer.convert_tokens_to_id("\n"):
                    j += 1

                # 找到对应的 <|im_end|>
                end_pos = None
                for k in range(j, n):
                    if input_ids[k] == assistant_end_id:
                        end_pos = k
                        break

                if end_pos is not None:
                    # 标记assistant内容为1（计算损失）
                    for pos in range(j, end_pos):
                        if pos < len(mask):
                            mask[pos] = 1

                i = end_pos + 1 if end_pos is not None else n
            else:
                i += 1

        return mask
```

**关键区别**：

- **预训练**：所有文本都参与损失计算
- **SFT**：只计算 AI 回答部分的损失，用户输入和系统提示不计算损失

## 5.4 模型构建

### 5.4.1 模型配置

```python
# code/k_model.py - ModelConfig类
class ModelConfig(PretrainedConfig):
    model_type = "Tiny-K"
    def __init__(
            self,
            dim: int = 768,           # 模型维度
            n_layers: int = 12,       # Transformer层数
            n_heads: int = 16,       # 注意力头数
            n_kv_heads: int = 8,     # 键值头数（GQA）
            vocab_size: int = 6144,   # 词汇表大小
            hidden_dim: int = None,   # 隐藏层维度
            multiple_of: int = 64,    # 维度倍数
            norm_eps: float = 1e-5,  # 归一化epsilon
            max_seq_len: int = 512,  # 最大序列长度
            dropout: float = 0.0,    # dropout概率
            flash_attn: bool = True, # 是否使用Flash Attention
            **kwargs,
    ):
        # 设置所有参数
        self.dim = dim
        self.n_layers = n_layers
        self.n_heads = n_heads
        self.n_kv_heads = n_kv_heads
        self.vocab_size = vocab_size
        self.hidden_dim = hidden_dim
        self.multiple_of = multiple_of
        self.norm_eps = norm_eps
        self.max_seq_len = max_seq_len
        self.dropout = dropout
        self.flash_attn = flash_attn
        super().__init__(**kwargs)
```

### 5.4.2 RMSNorm - 更快的归一化

```python
class RMSNorm(nn.Module):
    def __init__(self, dim: int, eps: float):
        super().__init__()
        self.eps = eps
        self.weight = nn.Parameter(torch.ones(dim))  # 可学习的缩放参数

    def _norm(self, x):
        # RMSNorm公式: x / sqrt(mean(x^2) + eps)
        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)

    def forward(self, x):
        # 先转为float计算，再转回原类型
        output = self._norm(x.float()).type_as(x)
        return output * self.weight
```

**为什么用 RMSNorm 而不是 LayerNorm？**

- 计算更快（少一次减法操作）
- 效果相当
- LLaMA2 系列都使用 RMSNorm

### 5.4.3 RoPE - 旋转位置编码

```python
def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):
    """预计算旋转位置的频率"""
    # 计算频率：1/(theta^(2i/dim))
    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))
    t = torch.arange(end, device=freqs.device)  # 位置索引
    freqs = torch.outer(t, freqs).float()  # 外积得到频率矩阵

    # 计算正弦和余弦
    freqs_cos = torch.cos(freqs)
    freqs_sin = torch.sin(freqs)
    return freqs_cos, freqs_sin

def apply_rotary_emb(xq, xk, freqs_cos, freqs_sin):
    """应用旋转位置编码"""
    # 将复数表示转换为实部和虚部
    xq_r, xq_i = xq.float().reshape(xq.shape[:-1] + (-1, 2)).unbind(-1)
    xk_r, xk_i = xk.float().reshape(xk.shape[:-1] + (-1, 2)).unbind(-1)

    # 应用旋转：复数乘法
    xq_out_r = xq_r * freqs_cos - xq_i * freqs_sin
    xq_out_i = xq_r * freqs_sin + xq_i * freqs_cos
    xk_out_r = xk_r * freqs_cos - xk_i * freqs_sin
    xk_out_i = xk_r * freqs_sin + xk_i * freqs_cos

    # 合并回原维度
    xq_out = torch.stack([xq_out_r, xq_out_i], dim=-1).flatten(3)
    xk_out = torch.stack([xk_out_r, xk_out_i], dim=-1).flatten(3)

    return xq_out.type_as(xq), xk_out.type_as(xk)
```

**RoPE 的优势**：

- 相对位置编码：能更好地处理长距离依赖
- 外推性好：能处理比训练时更长的序列
- 无参数：不需要学习的参数

### 5.4.4 注意力机制 - GQA（分组查询注意力）

```python
class Attention(nn.Module):
    def __init__(self, args: ModelConfig):
        super().__init__()
        # GQA：键值头数可以少于查询头数
        self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads
        assert args.n_heads % self.n_kv_heads == 0  # 确保能整除

        # 计算每个头的维度
        self.head_dim = args.dim // args.n_heads
        self.n_rep = args.n_heads // self.n_kv_heads  # 重复次数

        # 投影矩阵
        self.wq = nn.Linear(args.dim, args.n_heads * self.head_dim, bias=False)
        self.wk = nn.Linear(args.dim, self.n_kv_heads * self.head_dim, bias=False)
        self.wv = nn.Linear(args.dim, self.n_kv_heads * self.head_dim, bias=False)
        self.wo = nn.Linear(args.n_heads * self.head_dim, args.dim, bias=False)

        # 检查是否支持Flash Attention
        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')

    def forward(self, x: torch.Tensor, freqs_cos: torch.Tensor, freqs_sin: torch.Tensor):
        bsz, seqlen, _ = x.shape

        # 计算Q, K, V
        xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)

        # 重塑为多头形式
        xq = xq.view(bsz, seqlen, -1, self.head_dim).transpose(1, 2)
        xk = xk.view(bsz, seqlen, -1, self.head_dim).transpose(1, 2)
        xv = xv.view(bsz, seqlen, -1, self.head_dim).transpose(1, 2)

        # 应用RoPE
        xq, xk = apply_rotary_emb(xq, xk, freqs_cos, freqs_sin)

        # GQA：重复K, V以匹配Q的头数
        xk = repeat_kv(xk, self.n_rep)
        xv = repeat_kv(xv, self.n_rep)

        # 注意力计算
        if self.flash:
            # 使用Flash Attention（更快）
            output = torch.nn.functional.scaled_dot_product_attention(
                xq, xk, xv,
                attn_mask=None,
                dropout_p=self.dropout if self.training else 0.0,
                is_causal=True  # 因果掩码
            )
        else:
            # 手动实现
            scores = torch.matmul(xq, xk.transpose(-2, -1)) / math.sqrt(self.head_dim)
            scores = scores + self.mask[:, :, :seqlen, :seqlen]  # 因果掩码
            scores = F.softmax(scores.float(), dim=-1).type_as(xq)
            output = torch.matmul(scores, xv)

        # 输出投影
        output = output.transpose(1, 2).contiguous().view(bsz, seqlen, -1)
        return self.wo(output)
```

### 5.4.5 SwiGLU - 门控线性单元

**重要修正**：原文档中的 SwiGLU 实现是错误的！正确的实现如下：

```python
class MLP(nn.Module):
    def __init__(self, dim: int, hidden_dim: int, multiple_of: int, dropout: float):
        super().__init__()
        # 计算隐藏层维度
        if hidden_dim is None:
            hidden_dim = 4 * dim
            hidden_dim = int(2 * hidden_dim / 3)
            hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)

        # 三个线性层
        self.w1 = nn.Linear(dim, hidden_dim, bias=False)  # 门控层
        self.w2 = nn.Linear(dim, hidden_dim, bias=False)  # 输入层
        self.w3 = nn.Linear(hidden_dim, dim, bias=False)  # 输出层
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        # SwiGLU: Swish(x) * x
        # F.silu是Swish激活函数：x * sigmoid(x)
        return self.dropout(self.w3(F.silu(self.w1(x)) * self.w2(x)))
```

**为什么 SwiGLU 比 ReLU 好？**

- Swish 是平滑的激活函数，梯度流动更好
- 门控机制让网络学会选择性传递信息
- 在大模型中表现更好

### 5.4.6 Transformer 块

```python
class DecoderLayer(nn.Module):
    def __init__(self, layer_id: int, args: ModelConfig):
        super().__init__()
        self.attention = Attention(args)
        self.feed_forward = MLP(
            dim=args.dim,
            hidden_dim=args.hidden_dim,
            multiple_of=args.multiple_of,
            dropout=args.dropout,
        )
        # Pre-normalization架构
        self.attention_norm = RMSNorm(args.dim, eps=args.norm_eps)
        self.ffn_norm = RMSNorm(args.dim, eps=args.norm_eps)

    def forward(self, x, freqs_cos, freqs_sin):
        # 残差连接 + Pre-norm
        h = x + self.attention.forward(self.attention_norm(x), freqs_cos, freqs_sin)
        out = h + self.feed_forward.forward(self.ffn_norm(h))
        return out
```

### 5.4.7 完整模型

```python
class Transformer(PreTrainedModel):
    config_class = ModelConfig

    def __init__(self, args: ModelConfig = None):
        super().__init__(args)
        self.args = args

        # 词嵌入层
        self.tok_embeddings = nn.Embedding(args.vocab_size, args.dim)
        self.dropout = nn.Dropout(args.dropout)

        # Transformer层
        self.layers = torch.nn.ModuleList()
        for layer_id in range(args.n_layers):
            self.layers.append(DecoderLayer(layer_id, args))

        # 输出层
        self.norm = RMSNorm(args.dim, eps=args.norm_eps)
        self.output = nn.Linear(args.dim, args.vocab_size, bias=False)

        # 权重共享：词嵌入和输出层共享权重
        self.tok_embeddings.weight = self.output.weight

        # 预计算RoPE频率
        freqs_cos, freqs_sin = precompute_freqs_cis(
            self.args.dim // self.args.n_heads, self.args.max_seq_len
        )
        self.register_buffer("freqs_cos", freqs_cos, persistent=False)
        self.register_buffer("freqs_sin", freqs_sin, persistent=False)

        # 初始化权重
        self.apply(self._init_weights)

    def forward(self, tokens: torch.Tensor, targets: Optional[torch.Tensor] = None):
        _bsz, seqlen = tokens.shape

        # 词嵌入
        h = self.tok_embeddings(tokens)
        h = self.dropout(h)

        # 获取对应长度的RoPE
        freqs_cos = self.freqs_cos[:seqlen]
        freqs_sin = self.freqs_sin[:seqlen]

        # 通过所有Transformer层
        for layer in self.layers:
            h = layer(h, freqs_cos, freqs_sin)

        # 最终归一化
        h = self.norm(h)

        # 计算logits
        if targets is not None:
            # 训练时：计算所有位置的logits
            logits = self.output(h)
            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=0)
        else:
            # 推理时：只计算最后一个位置的logits（节省计算）
            logits = self.output(h[:, [-1], :])
            loss = None

        return {"logits": logits, "loss": loss}
```

## 5.5 训练过程

### 5.5.1 预训练

预训练的目标是让模型学习语言的基本规律。

```python
# code/train.py - 预训练部分
def train_epoch(epoch, train_loader, model, optimizer, scaler, ctx, args):
    start_time = time.time()
    iter_per_epoch = len(train_loader)

    for step, (X, Y, loss_mask) in enumerate(train_loader):
        X = X.to(args.device)  # 输入序列
        Y = Y.to(args.device)  # 目标序列
        loss_mask = loss_mask.to(args.device)  # 损失掩码

        # 学习率调度
        lr = get_lr(epoch * iter_per_epoch + step, args.epochs * iter_per_epoch, args)
        for param_group in optimizer.param_groups:
            param_group['lr'] = lr

        # 前向传播（混合精度）
        with ctx:
            out = model(X, Y)
            loss = out["loss"] / args.accumulation_steps  # 梯度累积
            loss_mask = loss_mask.view(-1)
            # 只计算有效位置的损失
            loss = torch.sum(loss * loss_mask) / loss_mask.sum()

        # 反向传播
        scaler.scale(loss).backward()

        # 梯度累积和更新
        if (step + 1) % args.accumulation_steps == 0:
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), args.grad_clip)
            scaler.step(optimizer)
            scaler.update()
            optimizer.zero_grad(set_to_none=True)

        # 日志输出
        if step % args.log_interval == 0:
            print(f"Epoch {epoch+1}, Step {step}, Loss: {loss.item():.3f}, LR: {lr:.7f}")

        # 保存模型
        if (step + 1) % args.save_interval == 0:
            save_model(model, f"pretrain_epoch{epoch}_step{step}.pth")
```

### 5.5.2 SFT 微调

SFT 微调让模型学会对话格式。

```python
# SFT训练与预训练类似，但使用不同的数据集
# 主要区别：
# 1. 数据格式不同（对话格式）
# 2. 损失计算不同（只计算assistant回复）
# 3. 通常从预训练权重开始

def init_model(args, lm_config):
    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_path)
    model = Transformer(lm_config)

    if args.mode == "sft" and args.pretrained_path:
        # 加载预训练权重
        state_dict = torch.load(args.pretrained_path, map_location=args.device)
        model.load_state_dict(state_dict, strict=False)
        print(f"Loaded pretrained model from {args.pretrained_path}")

    return model, tokenizer
```

### 5.5.3 训练技巧

1. **混合精度训练**：使用`torch.cuda.amp`加速训练并节省显存
2. **梯度累积**：模拟大批量训练，`accumulation_steps=8`相当于 batch_size×8
3. **梯度裁剪**：防止梯度爆炸，`grad_clip=1.0`
4. **学习率调度**：使用余弦退火，包含预热阶段
5. **权重初始化**：特别处理某些层的初始化

## 5.6 推理生成

### 5.6.1 生成算法

```python
# code/k_model.py - 生成方法
@torch.inference_mode()
def generate(self, idx, stop_id=None, max_new_tokens=256, temperature=1.0, top_k=None):
    """自回归生成文本"""
    index = idx.shape[1]  # 记录原始长度

    for _ in range(max_new_tokens):
        # 截断到最大长度
        idx_cond = idx if idx.size(1) <= self.args.max_seq_len else idx[:, -self.args.max_seq_len:]

        # 前向传播
        logits = self(idx_cond)["logits"]
        logits = logits[:, -1, :]  # 只取最后一个位置的logits

        # 温度控制
        if temperature == 0.0:
            # 贪婪解码
            _, idx_next = torch.topk(logits, k=1, dim=-1)
        else:
            # 温度采样
            logits = logits / temperature

            # Top-k采样
            if top_k is not None:
                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))
                logits[logits < v[:, [-1]]] = -float('Inf')

            # 采样
            probs = F.softmax(logits, dim=-1)
            idx_next = torch.multinomial(probs, num_samples=1)

        # 检查停止条件
        if stop_id is not None and idx_next == stop_id:
            break

        # 追加到序列
        idx = torch.cat((idx, idx_next), dim=1)

    return idx[:, index:]  # 只返回新生成的部分
```

### 5.6.2 使用示例

```python
# code/model_sample.py
class TextGenerator:
    def __init__(self, checkpoint_path, tokenizer_path):
        # 加载模型和分词器
        self.model = Transformer(ModelConfig(dim=1024, n_layers=18, vocab_size=6144))
        checkpoint = torch.load(checkpoint_path)
        self.model.load_state_dict(checkpoint)
        self.model.eval()

        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)

    def chat(self, prompt):
        """对话生成"""
        # 构建对话格式
        messages = [
            {"role": "system", "content": "你是一个AI助手"},
            {"role": "user", "content": prompt}
        ]
        text = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

        # 编码
        input_ids = self.tokenizer(text, return_tensors="pt").input_ids

        # 生成
        output = self.model.generate(
            input_ids,
            stop_id=self.tokenizer.eos_token_id,
            max_new_tokens=256,
            temperature=0.7,
            top_k=50
        )

        # 解码
        response = self.tokenizer.decode(output[0], skip_special_tokens=True)
        return response
```

## 5.7 完整使用流程

### 5.7.1 第一步：训练分词器

```bash
python code/train_tokenizer.py \
    --data_path your_corpus.jsonl \
    --save_dir tokenizer_k \
    --vocab_size 6144
```

### 5.7.2 第二步：预训练

```bash
python code/train.py \
    --mode pretrain \
    --data_path pretrain_data.jsonl \
    --tokenizer_path tokenizer_k \
    --out_dir output \
    --epochs 1 \
    --batch_size 4 \
    --learning_rate 2e-4 \
    --gpus 0,1,2,3
```

### 5.7.3 第三步：SFT 微调

```bash
python code/train.py \
    --mode sft \
    --data_path sft_data.jsonl \
    --tokenizer_path tokenizer_k \
    --pretrained_path output/pretrain_model.pth \
    --out_dir output \
    --epochs 3 \
    --batch_size 8 \
    --learning_rate 1e-5
```

### 5.7.4 第四步：推理测试

```bash
python code/model_sample.py
```

## 5.8 常见问题解答

### Q1: 为什么我的训练 loss 不下降？

A1: 可能的原因：

- 学习率太大或太小（建议 2e-4 到 1e-5）
- 数据有问题（检查数据格式）
- 模型初始化有问题（检查权重初始化）
- Batch size 太小（尝试增大或使用梯度累积）

### Q2: GPU 显存不足怎么办？

A2: 解决方案：

- 减小 batch_size
- 使用梯度累积
- 减小模型维度（dim=512 改为 256）
- 使用混合精度训练
- 使用多 GPU 训练

### Q3: 生成质量不好怎么办？

A3: 改进方法：

- 增加预训练数据量
- 增加模型参数量
- 调整生成参数（temperature, top_k）
- 进行更多的 SFT 训练
- 使用更好的预训练数据

### Q4: 训练时间太长怎么办？

A4: 加速方法：

- 使用多 GPU 训练
- 使用 Flash Attention（需要 PyTorch 2.0+）
- 增加 batch_size 和梯度累积
- 使用混合精度训练
- 减小模型规模

## 5.9 总结

本章我们完成了从零开始实现 LLaMA2 模型的整个过程：

1. **理解了每个组件的作用**：从分词器到 Transformer 块
2. **修正了常见错误**：特别是 SwiGLU 的正确实现
3. **掌握了训练技巧**：混合精度、梯度累积等
4. **学会了完整的流程**：从数据准备到模型部署

通过这个实践，你应该对大语言模型的内部工作原理有了更深入的理解。记住，真正的 AI 工程师不仅要会用 API，更要理解背后的原理！

**下一步建议**：

- 尝试在更大的数据集上训练
- 实现 LoRA 等参数高效微调方法
- 部署模型为 Web 服务
- 探索更多优化技术

祝你在 LLM 的道路上越走越远！🚀
