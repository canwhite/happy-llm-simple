# ç¬¬äº”ç« ï¼šåŠ¨æ‰‹æ­å»ºå¤§æ¨¡å‹ï¼ˆä¿®æ­£ç‰ˆï¼‰

## 5.1 å®ç°æµç¨‹æ¦‚è¿°

æœ¬ç« å°†å¸¦ä½ ä»é›¶å¼€å§‹å®ç°ä¸€ä¸ªåŸºäº LLaMA2 æ¶æ„çš„å¤§è¯­è¨€æ¨¡å‹ã€‚æ•´ä¸ªå®ç°è¿‡ç¨‹åˆ†ä¸ºä»¥ä¸‹æ­¥éª¤ï¼š

1. **è®­ç»ƒåˆ†è¯å™¨** - ç¬¬ä¸€æ­¥ï¼Œä¹Ÿæ˜¯æœ€åŸºç¡€çš„ä¸€æ­¥
2. **å‡†å¤‡æ•°æ®é›†** - åŒ…æ‹¬é¢„è®­ç»ƒæ•°æ®å’Œ SFT æ•°æ®
3. **æ„å»ºæ¨¡å‹** - å®ç° LLaMA2 çš„æ ¸å¿ƒç»„ä»¶
4. **é¢„è®­ç»ƒ** - åœ¨å¤§è§„æ¨¡è¯­æ–™ä¸Šè®­ç»ƒæ¨¡å‹
5. **SFT å¾®è°ƒ** - è®©æ¨¡å‹å­¦ä¼šå¯¹è¯
6. **æ¨ç†ç”Ÿæˆ** - ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹ç”Ÿæˆæ–‡æœ¬

## 5.2 è®­ç»ƒåˆ†è¯å™¨

### 5.2.1 ä¸ºä»€ä¹ˆå…ˆè®­ç»ƒåˆ†è¯å™¨ï¼Ÿ

åˆ†è¯å™¨æ˜¯è¯­è¨€æ¨¡å‹çš„åŸºç¡€ï¼Œå®ƒè´Ÿè´£å°†æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹èƒ½ç†è§£çš„æ•°å­—åºåˆ—ã€‚ä¸€ä¸ªå¥½çš„åˆ†è¯å™¨åº”è¯¥ï¼š

- èƒ½é«˜æ•ˆå¤„ç†å¤§é‡æ–‡æœ¬
- è¯æ±‡è¡¨å¤§å°é€‚ä¸­ï¼ˆå¤ªå¤§åˆ™æ¨¡å‹å‚æ•°å¤šï¼Œå¤ªå°åˆ™ä¿¡æ¯æŸå¤±å¤§ï¼‰
- æ”¯æŒç‰¹æ®Š tokenï¼ˆå¦‚å¯¹è¯å¼€å§‹/ç»“æŸæ ‡è®°ï¼‰

### 5.2.2 BPE ç®—æ³•ä»‹ç»

æˆ‘ä»¬é€‰æ‹© BPEï¼ˆByte Pair Encodingï¼‰ç®—æ³•æ¥è®­ç»ƒåˆ†è¯å™¨ã€‚BPE çš„ä¼˜ç‚¹ï¼š

- è‡ªåŠ¨å‘ç°å¸¸è§è¯å’Œå­è¯
- èƒ½å¤„ç†æœªç™»å½•è¯ï¼ˆOOV é—®é¢˜ï¼‰
- å‹ç¼©ç‡é«˜ï¼Œè¯æ±‡è¡¨å¤§å°å¯æ§

**æ ¸å¿ƒæ€æƒ³**ï¼šä¸æ–­åˆå¹¶æœ€é¢‘ç¹å‡ºç°çš„å­—ç¬¦å¯¹ï¼Œç›´åˆ°è¾¾åˆ°è¯æ±‡è¡¨å¤§å°é™åˆ¶ã€‚

### 5.2.3 å®ç°ä»£ç 

```python
# code/train_tokenizer.py
import argparse
import os
import random
import json
from typing import Generator
from transformers import AutoTokenizer
from tokenizers import (
    decoders,
    models,
    pre_tokenizers,
    trainers,
    Tokenizer,
)
from tokenizers.normalizers import NFKC

def train_tokenizer(data_path: str, save_dir: str, vocab_size: int = 6144) -> None:
    """è®­ç»ƒå¹¶ä¿å­˜è‡ªå®šä¹‰tokenizer"""
    os.makedirs(save_dir, exist_ok=True)

    # åˆå§‹åŒ–BPEåˆ†è¯å™¨
    tokenizer = Tokenizer(models.BPE(unk_token="<unk>"))
    tokenizer.normalizer = NFKC()  # æ–‡æœ¬è§„èŒƒåŒ–
    tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)
    tokenizer.decoder = decoders.ByteLevel()

    # å®šä¹‰ç‰¹æ®Štoken
    special_tokens = [
        "<unk>",     # æœªçŸ¥è¯
        "<s>",       # å¥å­å¼€å§‹
        "</s>",      # å¥å­ç»“æŸ
        "<|im_start|>",  # å¯¹è¯å¼€å§‹
        "<|im_end|>"     # å¯¹è¯ç»“æŸ
    ]

    # é…ç½®è®­ç»ƒå™¨
    trainer = trainers.BpeTrainer(
        vocab_size=vocab_size,
        special_tokens=special_tokens,
        min_frequency=2,  # è‡³å°‘å‡ºç°2æ¬¡æ‰çº³å…¥è¯æ±‡è¡¨
        show_progress=True,
        initial_alphabet=pre_tokenizers.ByteLevel.alphabet()
    )

    # å¼€å§‹è®­ç»ƒ
    print(f"Training tokenizer with data from {data_path}")
    texts = read_texts_from_jsonl(data_path)
    tokenizer.train_from_iterator(texts, trainer=trainer, length=os.path.getsize(data_path))

    # éªŒè¯ç‰¹æ®Štokenæ˜ å°„
    assert tokenizer.token_to_id("<unk>") == 0
    assert tokenizer.token_to_id("<s>") == 1
    assert tokenizer.token_to_id("</s>") == 2
    assert tokenizer.token_to_id("<|im_start|>") == 3
    assert tokenizer.token_to_id("<|im_end|>") == 4

    # ä¿å­˜åˆ†è¯å™¨
    tokenizer.save(os.path.join(save_dir, "tokenizer.json"))
    create_tokenizer_config(save_dir)
    print(f"Tokenizer saved to {save_dir}")
```

**å°ç™½è§£é‡Š**ï¼š

1. **åˆå§‹åŒ–**ï¼šåˆ›å»ºä¸€ä¸ª BPE åˆ†è¯å™¨ï¼Œè®¾ç½®æœªçŸ¥è¯æ ‡è®°
2. **æ–‡æœ¬é¢„å¤„ç†**ï¼šNFKC è§„èŒƒåŒ–æ–‡æœ¬ï¼ŒByteLevel é¢„å¤„ç†
3. **ç‰¹æ®Š token**ï¼šå®šä¹‰å¯¹è¯æ‰€éœ€çš„ç‰¹æ®Šæ ‡è®°
4. **è®­ç»ƒ**ï¼šä½¿ç”¨ BPE ç®—æ³•ä»æ–‡æœ¬ä¸­å­¦ä¹ è¯æ±‡è¡¨
5. **éªŒè¯**ï¼šç¡®ä¿ç‰¹æ®Š token çš„ ID æ˜ å°„æ­£ç¡®

### 5.2.4 ä½¿ç”¨è®­ç»ƒå¥½çš„åˆ†è¯å™¨

```bash
# è®­ç»ƒåˆ†è¯å™¨
python code/train_tokenizer.py --data_path your_data.jsonl --save_dir tokenizer_k --vocab_size 6144
```

## 5.3 æ•°æ®é›†å‡†å¤‡

### 5.3.1 é¢„è®­ç»ƒæ•°æ®é›†

é¢„è®­ç»ƒéœ€è¦å¤§é‡æ— æ ‡ç­¾çš„æ–‡æœ¬æ•°æ®ï¼Œæˆ‘ä»¬ä½¿ç”¨å‡ºé—¨é—®é—®åºåˆ—çŒ´å­æ•°æ®é›†ã€‚

```python
# code/dataset.py - PretrainDatasetç±»
class PretrainDataset(Dataset):
    def __init__(self, data_path, tokenizer, max_length=512):
        super().__init__()
        self.data_path = data_path
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.padding = tokenizer.pad_token_id if tokenizer.pad_token_id else 0

        with open(data_path, 'r', encoding='utf-8') as f:
            self.data = f.readlines()

    def __getitem__(self, index: int):
        # è¯»å–æ–‡æœ¬
        sample = json.loads(self.data[index])
        text = f"{self.tokenizer.bos_token if self.tokenizer.bos_token else ''}{sample['text']}"

        # åˆ†è¯å’Œæˆªæ–­
        input_id = self.tokenizer(text).data['input_ids'][:self.max_length]
        text_len = len(input_id)

        # å¡«å……
        padding_len = self.max_length - text_len
        input_id = input_id + [self.padding] * padding_len

        # ç”ŸæˆæŸå¤±æ©ç ï¼ˆ1=è®¡ç®—æŸå¤±ï¼Œ0=å¿½ç•¥ï¼‰
        loss_mask = [1] * text_len + [0] * padding_len

        # è½¬æ¢ä¸ºPyTorchå¼ é‡
        input_id = np.array(input_id)
        X = np.array(input_id[:-1]).astype(np.int64)  # è¾“å…¥ï¼šå»æ‰æœ€åä¸€ä¸ªtoken
        Y = np.array(input_id[1:]).astype(np.int64)  # æ ‡ç­¾ï¼šå»æ‰ç¬¬ä¸€ä¸ªtoken
        loss_mask = np.array(loss_mask[1:]).astype(np.int64)  # å¯¹åº”çš„æŸå¤±æ©ç 

        return torch.from_numpy(X), torch.from_numpy(Y), torch.from_numpy(loss_mask)
```

**å°ç™½è§£é‡Š**ï¼š

- **è¾“å…¥ X**ï¼šåºåˆ—çš„å‰ n-1 ä¸ª token
- **æ ‡ç­¾ Y**ï¼šåºåˆ—çš„å n-1 ä¸ª token
- **ç›®æ ‡**ï¼šè®©æ¨¡å‹å­¦ä¼šé¢„æµ‹ä¸‹ä¸€ä¸ªè¯
- **å¡«å……**ï¼šå°†çŸ­åºåˆ—å¡«å……åˆ°å›ºå®šé•¿åº¦
- **æŸå¤±æ©ç **ï¼šåªå¯¹çœŸå®æ–‡æœ¬è®¡ç®—æŸå¤±ï¼Œå¿½ç•¥å¡«å……éƒ¨åˆ†

### 5.3.2 SFT æ•°æ®é›†

SFTï¼ˆSupervised Fine-Tuningï¼‰æ•°æ®é›†æ˜¯æœ‰æ ‡ç­¾çš„å¯¹è¯æ•°æ®ã€‚

```python
# code/dataset.py - SFTDatasetç±»
class SFTDataset(Dataset):
    def __getitem__(self, index: int):
        sample = json.loads(self.data[index])

        # åº”ç”¨å¯¹è¯æ¨¡æ¿
        text = self.tokenizer.apply_chat_template(sample, tokenize=False, add_generation_prompt=False)
        input_id = self.tokenizer(text).data['input_ids'][:self.max_length]

        # ç”ŸæˆæŸå¤±æ©ç  - åªè®¡ç®—AIå›ç­”éƒ¨åˆ†çš„æŸå¤±
        loss_mask = self.generate_loss_mask(input_id)

        # ä¸é¢„è®­ç»ƒç±»ä¼¼çš„å¤„ç†...
        return torch.from_numpy(X), torch.from_numpy(Y), torch.from_numpy(loss_mask)

    def generate_loss_mask(self, input_ids):
        """ç”ŸæˆæŸå¤±æ©ç ï¼šåªè®¡ç®—assistantå›å¤çš„æŸå¤±"""
        mask = [0] * len(input_ids)

        # æŸ¥æ‰¾assistantå›å¤éƒ¨åˆ†
        assistant_start_id = self.tokenizer.convert_tokens_to_ids("<|im_start|>")
        assistant_end_id = self.tokenizer.convert_tokens_to_ids("<|im_end|>")

        i = 0
        n = len(input_ids)

        while i < n:
            # æ‰¾åˆ° <|im_start|>assistant
            if input_ids[i] == assistant_start_id and i + 2 < n and input_ids[i+2] == self.tokenizer.convert_tokens_to_ids("assistant"):
                # è·³åˆ°å†…å®¹å¼€å§‹
                j = i + 3
                if j < n and input_ids[j] == self.tokenizer.convert_tokens_to_id("\n"):
                    j += 1

                # æ‰¾åˆ°å¯¹åº”çš„ <|im_end|>
                end_pos = None
                for k in range(j, n):
                    if input_ids[k] == assistant_end_id:
                        end_pos = k
                        break

                if end_pos is not None:
                    # æ ‡è®°assistantå†…å®¹ä¸º1ï¼ˆè®¡ç®—æŸå¤±ï¼‰
                    for pos in range(j, end_pos):
                        if pos < len(mask):
                            mask[pos] = 1

                i = end_pos + 1 if end_pos is not None else n
            else:
                i += 1

        return mask
```

**å…³é”®åŒºåˆ«**ï¼š

- **é¢„è®­ç»ƒ**ï¼šæ‰€æœ‰æ–‡æœ¬éƒ½å‚ä¸æŸå¤±è®¡ç®—
- **SFT**ï¼šåªè®¡ç®— AI å›ç­”éƒ¨åˆ†çš„æŸå¤±ï¼Œç”¨æˆ·è¾“å…¥å’Œç³»ç»Ÿæç¤ºä¸è®¡ç®—æŸå¤±

## 5.4 æ¨¡å‹æ„å»º

### 5.4.1 æ¨¡å‹é…ç½®

```python
# code/k_model.py - ModelConfigç±»
class ModelConfig(PretrainedConfig):
    model_type = "Tiny-K"
    def __init__(
            self,
            dim: int = 768,           # æ¨¡å‹ç»´åº¦
            n_layers: int = 12,       # Transformerå±‚æ•°
            n_heads: int = 16,       # æ³¨æ„åŠ›å¤´æ•°
            n_kv_heads: int = 8,     # é”®å€¼å¤´æ•°ï¼ˆGQAï¼‰
            vocab_size: int = 6144,   # è¯æ±‡è¡¨å¤§å°
            hidden_dim: int = None,   # éšè—å±‚ç»´åº¦
            multiple_of: int = 64,    # ç»´åº¦å€æ•°
            norm_eps: float = 1e-5,  # å½’ä¸€åŒ–epsilon
            max_seq_len: int = 512,  # æœ€å¤§åºåˆ—é•¿åº¦
            dropout: float = 0.0,    # dropoutæ¦‚ç‡
            flash_attn: bool = True, # æ˜¯å¦ä½¿ç”¨Flash Attention
            **kwargs,
    ):
        # è®¾ç½®æ‰€æœ‰å‚æ•°
        self.dim = dim
        self.n_layers = n_layers
        self.n_heads = n_heads
        self.n_kv_heads = n_kv_heads
        self.vocab_size = vocab_size
        self.hidden_dim = hidden_dim
        self.multiple_of = multiple_of
        self.norm_eps = norm_eps
        self.max_seq_len = max_seq_len
        self.dropout = dropout
        self.flash_attn = flash_attn
        super().__init__(**kwargs)
```

### 5.4.2 RMSNorm - æ›´å¿«çš„å½’ä¸€åŒ–

```python
class RMSNorm(nn.Module):
    def __init__(self, dim: int, eps: float):
        super().__init__()
        self.eps = eps
        self.weight = nn.Parameter(torch.ones(dim))  # å¯å­¦ä¹ çš„ç¼©æ”¾å‚æ•°

    def _norm(self, x):
        # RMSNormå…¬å¼: x / sqrt(mean(x^2) + eps)
        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)

    def forward(self, x):
        # å…ˆè½¬ä¸ºfloatè®¡ç®—ï¼Œå†è½¬å›åŸç±»å‹
        output = self._norm(x.float()).type_as(x)
        return output * self.weight
```

**ä¸ºä»€ä¹ˆç”¨ RMSNorm è€Œä¸æ˜¯ LayerNormï¼Ÿ**

- è®¡ç®—æ›´å¿«ï¼ˆå°‘ä¸€æ¬¡å‡æ³•æ“ä½œï¼‰
- æ•ˆæœç›¸å½“
- LLaMA2 ç³»åˆ—éƒ½ä½¿ç”¨ RMSNorm

### 5.4.3 RoPE - æ—‹è½¬ä½ç½®ç¼–ç 

```python
def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):
    """é¢„è®¡ç®—æ—‹è½¬ä½ç½®çš„é¢‘ç‡"""
    # è®¡ç®—é¢‘ç‡ï¼š1/(theta^(2i/dim))
    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))
    t = torch.arange(end, device=freqs.device)  # ä½ç½®ç´¢å¼•
    freqs = torch.outer(t, freqs).float()  # å¤–ç§¯å¾—åˆ°é¢‘ç‡çŸ©é˜µ

    # è®¡ç®—æ­£å¼¦å’Œä½™å¼¦
    freqs_cos = torch.cos(freqs)
    freqs_sin = torch.sin(freqs)
    return freqs_cos, freqs_sin

def apply_rotary_emb(xq, xk, freqs_cos, freqs_sin):
    """åº”ç”¨æ—‹è½¬ä½ç½®ç¼–ç """
    # å°†å¤æ•°è¡¨ç¤ºè½¬æ¢ä¸ºå®éƒ¨å’Œè™šéƒ¨
    xq_r, xq_i = xq.float().reshape(xq.shape[:-1] + (-1, 2)).unbind(-1)
    xk_r, xk_i = xk.float().reshape(xk.shape[:-1] + (-1, 2)).unbind(-1)

    # åº”ç”¨æ—‹è½¬ï¼šå¤æ•°ä¹˜æ³•
    xq_out_r = xq_r * freqs_cos - xq_i * freqs_sin
    xq_out_i = xq_r * freqs_sin + xq_i * freqs_cos
    xk_out_r = xk_r * freqs_cos - xk_i * freqs_sin
    xk_out_i = xk_r * freqs_sin + xk_i * freqs_cos

    # åˆå¹¶å›åŸç»´åº¦
    xq_out = torch.stack([xq_out_r, xq_out_i], dim=-1).flatten(3)
    xk_out = torch.stack([xk_out_r, xk_out_i], dim=-1).flatten(3)

    return xq_out.type_as(xq), xk_out.type_as(xk)
```

**RoPE çš„ä¼˜åŠ¿**ï¼š

- ç›¸å¯¹ä½ç½®ç¼–ç ï¼šèƒ½æ›´å¥½åœ°å¤„ç†é•¿è·ç¦»ä¾èµ–
- å¤–æ¨æ€§å¥½ï¼šèƒ½å¤„ç†æ¯”è®­ç»ƒæ—¶æ›´é•¿çš„åºåˆ—
- æ— å‚æ•°ï¼šä¸éœ€è¦å­¦ä¹ çš„å‚æ•°

### 5.4.4 æ³¨æ„åŠ›æœºåˆ¶ - GQAï¼ˆåˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›ï¼‰

```python
class Attention(nn.Module):
    def __init__(self, args: ModelConfig):
        super().__init__()
        # GQAï¼šé”®å€¼å¤´æ•°å¯ä»¥å°‘äºæŸ¥è¯¢å¤´æ•°
        self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads
        assert args.n_heads % self.n_kv_heads == 0  # ç¡®ä¿èƒ½æ•´é™¤

        # è®¡ç®—æ¯ä¸ªå¤´çš„ç»´åº¦
        self.head_dim = args.dim // args.n_heads
        self.n_rep = args.n_heads // self.n_kv_heads  # é‡å¤æ¬¡æ•°

        # æŠ•å½±çŸ©é˜µ
        self.wq = nn.Linear(args.dim, args.n_heads * self.head_dim, bias=False)
        self.wk = nn.Linear(args.dim, self.n_kv_heads * self.head_dim, bias=False)
        self.wv = nn.Linear(args.dim, self.n_kv_heads * self.head_dim, bias=False)
        self.wo = nn.Linear(args.n_heads * self.head_dim, args.dim, bias=False)

        # æ£€æŸ¥æ˜¯å¦æ”¯æŒFlash Attention
        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')

    def forward(self, x: torch.Tensor, freqs_cos: torch.Tensor, freqs_sin: torch.Tensor):
        bsz, seqlen, _ = x.shape

        # è®¡ç®—Q, K, V
        xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)

        # é‡å¡‘ä¸ºå¤šå¤´å½¢å¼
        xq = xq.view(bsz, seqlen, -1, self.head_dim).transpose(1, 2)
        xk = xk.view(bsz, seqlen, -1, self.head_dim).transpose(1, 2)
        xv = xv.view(bsz, seqlen, -1, self.head_dim).transpose(1, 2)

        # åº”ç”¨RoPE
        xq, xk = apply_rotary_emb(xq, xk, freqs_cos, freqs_sin)

        # GQAï¼šé‡å¤K, Vä»¥åŒ¹é…Qçš„å¤´æ•°
        xk = repeat_kv(xk, self.n_rep)
        xv = repeat_kv(xv, self.n_rep)

        # æ³¨æ„åŠ›è®¡ç®—
        if self.flash:
            # ä½¿ç”¨Flash Attentionï¼ˆæ›´å¿«ï¼‰
            output = torch.nn.functional.scaled_dot_product_attention(
                xq, xk, xv,
                attn_mask=None,
                dropout_p=self.dropout if self.training else 0.0,
                is_causal=True  # å› æœæ©ç 
            )
        else:
            # æ‰‹åŠ¨å®ç°
            scores = torch.matmul(xq, xk.transpose(-2, -1)) / math.sqrt(self.head_dim)
            scores = scores + self.mask[:, :, :seqlen, :seqlen]  # å› æœæ©ç 
            scores = F.softmax(scores.float(), dim=-1).type_as(xq)
            output = torch.matmul(scores, xv)

        # è¾“å‡ºæŠ•å½±
        output = output.transpose(1, 2).contiguous().view(bsz, seqlen, -1)
        return self.wo(output)
```

### 5.4.5 SwiGLU - é—¨æ§çº¿æ€§å•å…ƒ

**é‡è¦ä¿®æ­£**ï¼šåŸæ–‡æ¡£ä¸­çš„ SwiGLU å®ç°æ˜¯é”™è¯¯çš„ï¼æ­£ç¡®çš„å®ç°å¦‚ä¸‹ï¼š

```python
class MLP(nn.Module):
    def __init__(self, dim: int, hidden_dim: int, multiple_of: int, dropout: float):
        super().__init__()
        # è®¡ç®—éšè—å±‚ç»´åº¦
        if hidden_dim is None:
            hidden_dim = 4 * dim
            hidden_dim = int(2 * hidden_dim / 3)
            hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)

        # ä¸‰ä¸ªçº¿æ€§å±‚
        self.w1 = nn.Linear(dim, hidden_dim, bias=False)  # é—¨æ§å±‚
        self.w2 = nn.Linear(dim, hidden_dim, bias=False)  # è¾“å…¥å±‚
        self.w3 = nn.Linear(hidden_dim, dim, bias=False)  # è¾“å‡ºå±‚
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        # SwiGLU: Swish(x) * x
        # F.siluæ˜¯Swishæ¿€æ´»å‡½æ•°ï¼šx * sigmoid(x)
        return self.dropout(self.w3(F.silu(self.w1(x)) * self.w2(x)))
```

**ä¸ºä»€ä¹ˆ SwiGLU æ¯” ReLU å¥½ï¼Ÿ**

- Swish æ˜¯å¹³æ»‘çš„æ¿€æ´»å‡½æ•°ï¼Œæ¢¯åº¦æµåŠ¨æ›´å¥½
- é—¨æ§æœºåˆ¶è®©ç½‘ç»œå­¦ä¼šé€‰æ‹©æ€§ä¼ é€’ä¿¡æ¯
- åœ¨å¤§æ¨¡å‹ä¸­è¡¨ç°æ›´å¥½

### 5.4.6 Transformer å—

```python
class DecoderLayer(nn.Module):
    def __init__(self, layer_id: int, args: ModelConfig):
        super().__init__()
        self.attention = Attention(args)
        self.feed_forward = MLP(
            dim=args.dim,
            hidden_dim=args.hidden_dim,
            multiple_of=args.multiple_of,
            dropout=args.dropout,
        )
        # Pre-normalizationæ¶æ„
        self.attention_norm = RMSNorm(args.dim, eps=args.norm_eps)
        self.ffn_norm = RMSNorm(args.dim, eps=args.norm_eps)

    def forward(self, x, freqs_cos, freqs_sin):
        # æ®‹å·®è¿æ¥ + Pre-norm
        h = x + self.attention.forward(self.attention_norm(x), freqs_cos, freqs_sin)
        out = h + self.feed_forward.forward(self.ffn_norm(h))
        return out
```

### 5.4.7 å®Œæ•´æ¨¡å‹

```python
class Transformer(PreTrainedModel):
    config_class = ModelConfig

    def __init__(self, args: ModelConfig = None):
        super().__init__(args)
        self.args = args

        # è¯åµŒå…¥å±‚
        self.tok_embeddings = nn.Embedding(args.vocab_size, args.dim)
        self.dropout = nn.Dropout(args.dropout)

        # Transformerå±‚
        self.layers = torch.nn.ModuleList()
        for layer_id in range(args.n_layers):
            self.layers.append(DecoderLayer(layer_id, args))

        # è¾“å‡ºå±‚
        self.norm = RMSNorm(args.dim, eps=args.norm_eps)
        self.output = nn.Linear(args.dim, args.vocab_size, bias=False)

        # æƒé‡å…±äº«ï¼šè¯åµŒå…¥å’Œè¾“å‡ºå±‚å…±äº«æƒé‡
        self.tok_embeddings.weight = self.output.weight

        # é¢„è®¡ç®—RoPEé¢‘ç‡
        freqs_cos, freqs_sin = precompute_freqs_cis(
            self.args.dim // self.args.n_heads, self.args.max_seq_len
        )
        self.register_buffer("freqs_cos", freqs_cos, persistent=False)
        self.register_buffer("freqs_sin", freqs_sin, persistent=False)

        # åˆå§‹åŒ–æƒé‡
        self.apply(self._init_weights)

    def forward(self, tokens: torch.Tensor, targets: Optional[torch.Tensor] = None):
        _bsz, seqlen = tokens.shape

        # è¯åµŒå…¥
        h = self.tok_embeddings(tokens)
        h = self.dropout(h)

        # è·å–å¯¹åº”é•¿åº¦çš„RoPE
        freqs_cos = self.freqs_cos[:seqlen]
        freqs_sin = self.freqs_sin[:seqlen]

        # é€šè¿‡æ‰€æœ‰Transformerå±‚
        for layer in self.layers:
            h = layer(h, freqs_cos, freqs_sin)

        # æœ€ç»ˆå½’ä¸€åŒ–
        h = self.norm(h)

        # è®¡ç®—logits
        if targets is not None:
            # è®­ç»ƒæ—¶ï¼šè®¡ç®—æ‰€æœ‰ä½ç½®çš„logits
            logits = self.output(h)
            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=0)
        else:
            # æ¨ç†æ—¶ï¼šåªè®¡ç®—æœ€åä¸€ä¸ªä½ç½®çš„logitsï¼ˆèŠ‚çœè®¡ç®—ï¼‰
            logits = self.output(h[:, [-1], :])
            loss = None

        return {"logits": logits, "loss": loss}
```

## 5.5 è®­ç»ƒè¿‡ç¨‹

### 5.5.1 é¢„è®­ç»ƒ

é¢„è®­ç»ƒçš„ç›®æ ‡æ˜¯è®©æ¨¡å‹å­¦ä¹ è¯­è¨€çš„åŸºæœ¬è§„å¾‹ã€‚

```python
# code/train.py - é¢„è®­ç»ƒéƒ¨åˆ†
def train_epoch(epoch, train_loader, model, optimizer, scaler, ctx, args):
    start_time = time.time()
    iter_per_epoch = len(train_loader)

    for step, (X, Y, loss_mask) in enumerate(train_loader):
        X = X.to(args.device)  # è¾“å…¥åºåˆ—
        Y = Y.to(args.device)  # ç›®æ ‡åºåˆ—
        loss_mask = loss_mask.to(args.device)  # æŸå¤±æ©ç 

        # å­¦ä¹ ç‡è°ƒåº¦
        lr = get_lr(epoch * iter_per_epoch + step, args.epochs * iter_per_epoch, args)
        for param_group in optimizer.param_groups:
            param_group['lr'] = lr

        # å‰å‘ä¼ æ’­ï¼ˆæ··åˆç²¾åº¦ï¼‰
        with ctx:
            out = model(X, Y)
            loss = out["loss"] / args.accumulation_steps  # æ¢¯åº¦ç´¯ç§¯
            loss_mask = loss_mask.view(-1)
            # åªè®¡ç®—æœ‰æ•ˆä½ç½®çš„æŸå¤±
            loss = torch.sum(loss * loss_mask) / loss_mask.sum()

        # åå‘ä¼ æ’­
        scaler.scale(loss).backward()

        # æ¢¯åº¦ç´¯ç§¯å’Œæ›´æ–°
        if (step + 1) % args.accumulation_steps == 0:
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), args.grad_clip)
            scaler.step(optimizer)
            scaler.update()
            optimizer.zero_grad(set_to_none=True)

        # æ—¥å¿—è¾“å‡º
        if step % args.log_interval == 0:
            print(f"Epoch {epoch+1}, Step {step}, Loss: {loss.item():.3f}, LR: {lr:.7f}")

        # ä¿å­˜æ¨¡å‹
        if (step + 1) % args.save_interval == 0:
            save_model(model, f"pretrain_epoch{epoch}_step{step}.pth")
```

### 5.5.2 SFT å¾®è°ƒ

SFT å¾®è°ƒè®©æ¨¡å‹å­¦ä¼šå¯¹è¯æ ¼å¼ã€‚

```python
# SFTè®­ç»ƒä¸é¢„è®­ç»ƒç±»ä¼¼ï¼Œä½†ä½¿ç”¨ä¸åŒçš„æ•°æ®é›†
# ä¸»è¦åŒºåˆ«ï¼š
# 1. æ•°æ®æ ¼å¼ä¸åŒï¼ˆå¯¹è¯æ ¼å¼ï¼‰
# 2. æŸå¤±è®¡ç®—ä¸åŒï¼ˆåªè®¡ç®—assistantå›å¤ï¼‰
# 3. é€šå¸¸ä»é¢„è®­ç»ƒæƒé‡å¼€å§‹

def init_model(args, lm_config):
    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_path)
    model = Transformer(lm_config)

    if args.mode == "sft" and args.pretrained_path:
        # åŠ è½½é¢„è®­ç»ƒæƒé‡
        state_dict = torch.load(args.pretrained_path, map_location=args.device)
        model.load_state_dict(state_dict, strict=False)
        print(f"Loaded pretrained model from {args.pretrained_path}")

    return model, tokenizer
```

### 5.5.3 è®­ç»ƒæŠ€å·§

1. **æ··åˆç²¾åº¦è®­ç»ƒ**ï¼šä½¿ç”¨`torch.cuda.amp`åŠ é€Ÿè®­ç»ƒå¹¶èŠ‚çœæ˜¾å­˜
2. **æ¢¯åº¦ç´¯ç§¯**ï¼šæ¨¡æ‹Ÿå¤§æ‰¹é‡è®­ç»ƒï¼Œ`accumulation_steps=8`ç›¸å½“äº batch_sizeÃ—8
3. **æ¢¯åº¦è£å‰ª**ï¼šé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼Œ`grad_clip=1.0`
4. **å­¦ä¹ ç‡è°ƒåº¦**ï¼šä½¿ç”¨ä½™å¼¦é€€ç«ï¼ŒåŒ…å«é¢„çƒ­é˜¶æ®µ
5. **æƒé‡åˆå§‹åŒ–**ï¼šç‰¹åˆ«å¤„ç†æŸäº›å±‚çš„åˆå§‹åŒ–

## 5.6 æ¨ç†ç”Ÿæˆ

### 5.6.1 ç”Ÿæˆç®—æ³•

```python
# code/k_model.py - ç”Ÿæˆæ–¹æ³•
@torch.inference_mode()
def generate(self, idx, stop_id=None, max_new_tokens=256, temperature=1.0, top_k=None):
    """è‡ªå›å½’ç”Ÿæˆæ–‡æœ¬"""
    index = idx.shape[1]  # è®°å½•åŸå§‹é•¿åº¦

    for _ in range(max_new_tokens):
        # æˆªæ–­åˆ°æœ€å¤§é•¿åº¦
        idx_cond = idx if idx.size(1) <= self.args.max_seq_len else idx[:, -self.args.max_seq_len:]

        # å‰å‘ä¼ æ’­
        logits = self(idx_cond)["logits"]
        logits = logits[:, -1, :]  # åªå–æœ€åä¸€ä¸ªä½ç½®çš„logits

        # æ¸©åº¦æ§åˆ¶
        if temperature == 0.0:
            # è´ªå©ªè§£ç 
            _, idx_next = torch.topk(logits, k=1, dim=-1)
        else:
            # æ¸©åº¦é‡‡æ ·
            logits = logits / temperature

            # Top-ké‡‡æ ·
            if top_k is not None:
                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))
                logits[logits < v[:, [-1]]] = -float('Inf')

            # é‡‡æ ·
            probs = F.softmax(logits, dim=-1)
            idx_next = torch.multinomial(probs, num_samples=1)

        # æ£€æŸ¥åœæ­¢æ¡ä»¶
        if stop_id is not None and idx_next == stop_id:
            break

        # è¿½åŠ åˆ°åºåˆ—
        idx = torch.cat((idx, idx_next), dim=1)

    return idx[:, index:]  # åªè¿”å›æ–°ç”Ÿæˆçš„éƒ¨åˆ†
```

### 5.6.2 ä½¿ç”¨ç¤ºä¾‹

```python
# code/model_sample.py
class TextGenerator:
    def __init__(self, checkpoint_path, tokenizer_path):
        # åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
        self.model = Transformer(ModelConfig(dim=1024, n_layers=18, vocab_size=6144))
        checkpoint = torch.load(checkpoint_path)
        self.model.load_state_dict(checkpoint)
        self.model.eval()

        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)

    def chat(self, prompt):
        """å¯¹è¯ç”Ÿæˆ"""
        # æ„å»ºå¯¹è¯æ ¼å¼
        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªAIåŠ©æ‰‹"},
            {"role": "user", "content": prompt}
        ]
        text = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

        # ç¼–ç 
        input_ids = self.tokenizer(text, return_tensors="pt").input_ids

        # ç”Ÿæˆ
        output = self.model.generate(
            input_ids,
            stop_id=self.tokenizer.eos_token_id,
            max_new_tokens=256,
            temperature=0.7,
            top_k=50
        )

        # è§£ç 
        response = self.tokenizer.decode(output[0], skip_special_tokens=True)
        return response
```

## 5.7 å®Œæ•´ä½¿ç”¨æµç¨‹

### 5.7.1 ç¬¬ä¸€æ­¥ï¼šè®­ç»ƒåˆ†è¯å™¨

```bash
python code/train_tokenizer.py \
    --data_path your_corpus.jsonl \
    --save_dir tokenizer_k \
    --vocab_size 6144
```

### 5.7.2 ç¬¬äºŒæ­¥ï¼šé¢„è®­ç»ƒ

```bash
python code/train.py \
    --mode pretrain \
    --data_path pretrain_data.jsonl \
    --tokenizer_path tokenizer_k \
    --out_dir output \
    --epochs 1 \
    --batch_size 4 \
    --learning_rate 2e-4 \
    --gpus 0,1,2,3
```

### 5.7.3 ç¬¬ä¸‰æ­¥ï¼šSFT å¾®è°ƒ

```bash
python code/train.py \
    --mode sft \
    --data_path sft_data.jsonl \
    --tokenizer_path tokenizer_k \
    --pretrained_path output/pretrain_model.pth \
    --out_dir output \
    --epochs 3 \
    --batch_size 8 \
    --learning_rate 1e-5
```

### 5.7.4 ç¬¬å››æ­¥ï¼šæ¨ç†æµ‹è¯•

```bash
python code/model_sample.py
```

## 5.8 å¸¸è§é—®é¢˜è§£ç­”

### Q1: ä¸ºä»€ä¹ˆæˆ‘çš„è®­ç»ƒ loss ä¸ä¸‹é™ï¼Ÿ

A1: å¯èƒ½çš„åŸå› ï¼š

- å­¦ä¹ ç‡å¤ªå¤§æˆ–å¤ªå°ï¼ˆå»ºè®® 2e-4 åˆ° 1e-5ï¼‰
- æ•°æ®æœ‰é—®é¢˜ï¼ˆæ£€æŸ¥æ•°æ®æ ¼å¼ï¼‰
- æ¨¡å‹åˆå§‹åŒ–æœ‰é—®é¢˜ï¼ˆæ£€æŸ¥æƒé‡åˆå§‹åŒ–ï¼‰
- Batch size å¤ªå°ï¼ˆå°è¯•å¢å¤§æˆ–ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯ï¼‰

### Q2: GPU æ˜¾å­˜ä¸è¶³æ€ä¹ˆåŠï¼Ÿ

A2: è§£å†³æ–¹æ¡ˆï¼š

- å‡å° batch_size
- ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯
- å‡å°æ¨¡å‹ç»´åº¦ï¼ˆdim=512 æ”¹ä¸º 256ï¼‰
- ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
- ä½¿ç”¨å¤š GPU è®­ç»ƒ

### Q3: ç”Ÿæˆè´¨é‡ä¸å¥½æ€ä¹ˆåŠï¼Ÿ

A3: æ”¹è¿›æ–¹æ³•ï¼š

- å¢åŠ é¢„è®­ç»ƒæ•°æ®é‡
- å¢åŠ æ¨¡å‹å‚æ•°é‡
- è°ƒæ•´ç”Ÿæˆå‚æ•°ï¼ˆtemperature, top_kï¼‰
- è¿›è¡Œæ›´å¤šçš„ SFT è®­ç»ƒ
- ä½¿ç”¨æ›´å¥½çš„é¢„è®­ç»ƒæ•°æ®

### Q4: è®­ç»ƒæ—¶é—´å¤ªé•¿æ€ä¹ˆåŠï¼Ÿ

A4: åŠ é€Ÿæ–¹æ³•ï¼š

- ä½¿ç”¨å¤š GPU è®­ç»ƒ
- ä½¿ç”¨ Flash Attentionï¼ˆéœ€è¦ PyTorch 2.0+ï¼‰
- å¢åŠ  batch_size å’Œæ¢¯åº¦ç´¯ç§¯
- ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
- å‡å°æ¨¡å‹è§„æ¨¡

## 5.9 æ€»ç»“

æœ¬ç« æˆ‘ä»¬å®Œæˆäº†ä»é›¶å¼€å§‹å®ç° LLaMA2 æ¨¡å‹çš„æ•´ä¸ªè¿‡ç¨‹ï¼š

1. **ç†è§£äº†æ¯ä¸ªç»„ä»¶çš„ä½œç”¨**ï¼šä»åˆ†è¯å™¨åˆ° Transformer å—
2. **ä¿®æ­£äº†å¸¸è§é”™è¯¯**ï¼šç‰¹åˆ«æ˜¯ SwiGLU çš„æ­£ç¡®å®ç°
3. **æŒæ¡äº†è®­ç»ƒæŠ€å·§**ï¼šæ··åˆç²¾åº¦ã€æ¢¯åº¦ç´¯ç§¯ç­‰
4. **å­¦ä¼šäº†å®Œæ•´çš„æµç¨‹**ï¼šä»æ•°æ®å‡†å¤‡åˆ°æ¨¡å‹éƒ¨ç½²

é€šè¿‡è¿™ä¸ªå®è·µï¼Œä½ åº”è¯¥å¯¹å¤§è¯­è¨€æ¨¡å‹çš„å†…éƒ¨å·¥ä½œåŸç†æœ‰äº†æ›´æ·±å…¥çš„ç†è§£ã€‚è®°ä½ï¼ŒçœŸæ­£çš„ AI å·¥ç¨‹å¸ˆä¸ä»…è¦ä¼šç”¨ APIï¼Œæ›´è¦ç†è§£èƒŒåçš„åŸç†ï¼

**ä¸‹ä¸€æ­¥å»ºè®®**ï¼š

- å°è¯•åœ¨æ›´å¤§çš„æ•°æ®é›†ä¸Šè®­ç»ƒ
- å®ç° LoRA ç­‰å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•
- éƒ¨ç½²æ¨¡å‹ä¸º Web æœåŠ¡
- æ¢ç´¢æ›´å¤šä¼˜åŒ–æŠ€æœ¯

ç¥ä½ åœ¨ LLM çš„é“è·¯ä¸Šè¶Šèµ°è¶Šè¿œï¼ğŸš€
