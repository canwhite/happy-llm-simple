## Happy-LLM-Simple

---

[ä¸­æ–‡](#ä¸­æ–‡) | [English](#english)

---

<div id="ä¸­æ–‡">

### é¡¹ç›®ç®€ä»‹

10 æœˆ 14 æ—¥æ™šä¸ŠèŠ±äº†ä¸¤ä¸ªå°æ—¶å­¦ä¹ [Happy-LLM](https://github.com/datawhalechina/happy-llm)åï¼Œä¸ºäº†æ–¹ä¾¿ç†è§£å¤ç›˜å¼€äº†è¿™ä¸ªä»“åº“ã€‚ç›¸å¯¹åŸæœ¬æ›´åŠ æ˜æ™°ç®€çŸ­ï¼Œå¯ä»¥ç†è§£ä¸ºæ— ç—›å°ç™½ç‰ˆï¼Œæ•´ä½“è¯»å®Œå¤§æ¦‚ä¸‰å››ååˆ†é’Ÿå·¦å³ã€‚å¸Œæœ›å¯¹å¤§å®¶æœ‰å¸®åŠ©ã€‚

10 æœˆ 24 æ—¥æ›´æ–°äº† PyTorch ç›¸å…³ä¿¡æ¯ã€‚

## ğŸ“– ç›®å½•å¯¼èˆª

| ç« èŠ‚                                       | ä¸»è¦å†…å®¹                                             | çŠ¶æ€ |
| ------------------------------------------ | ---------------------------------------------------- | ---- |
| [å‰è¨€](./0.Pre.md)                         | é¡¹ç›®èµ·æºã€èƒŒæ™¯ä»‹ç»å’Œè¯»è€…å»ºè®®                         | âœ…   |
| [ç¬¬ 1 ç« ï¼šNLP](./1.NLP.md)                 | ä»€ä¹ˆæ˜¯ NLPã€å‘å±•å†ç¨‹ã€ä»»åŠ¡åˆ†ç±»ã€æ–‡æœ¬è¡¨ç¤ºæ¼”è¿›         | âœ…   |
| [ç¬¬ 2 ç« ï¼šTransformer](./2.Transformer.md) | æ³¨æ„åŠ›æœºåˆ¶ã€ç¼–ç å™¨-è§£ç å™¨ã€åŠ¨æ‰‹æ„å»º Transformer      | âœ…   |
| [ç¬¬ 3 ç« ï¼šPLM](./3.PLM.md)                 | Encoder-onlyã€Encoder-Decoderã€Decoder-Only æ¨¡å‹å¯¹æ¯” | âœ…   |
| [ç¬¬ 4 ç« ï¼šLLM](./4.LLM.md)                 | LLM å®šä¹‰ã€è®­ç»ƒç­–ç•¥ã€æ¶Œç°èƒ½åŠ›åˆ†æ                     | âœ…   |
| [ç¬¬ 5 ç« ï¼šLLM å®ç°](./5.LLM_Implement.md)  | å®ç° LLaMA2ã€è®­ç»ƒ Tokenizerã€é¢„è®­ç»ƒå°è§„æ¨¡ LLM        | âœ…   |
| [ç¬¬ 6 ç« ï¼šè®­ç»ƒ](./6.Train.md)              | é¢„è®­ç»ƒã€ç›‘ç£å¾®è°ƒã€LoRA/QLoRA é«˜æ•ˆå¾®è°ƒ                | ğŸš§   |
| [ç¬¬ 7 ç« ï¼šåº”ç”¨](./7.Usage.md)              | æ¨¡å‹è¯„ä¼°ã€RAG æ£€ç´¢å¢å¼ºã€Agent æ™ºèƒ½ä½“                 | âœ…   |
| [PyTorch åŸºç¡€](./PyTorch.md)               | PyTorch è®­ç»ƒæµç¨‹ã€åå‘ä¼ æ’­ã€æ¢¯åº¦ä¸‹é™ã€æ¶æ„å¯¹æ¯”        | âœ…   |

### ç‰¹åˆ«æ„Ÿè°¢

[Happy-LLM](https://github.com/datawhalechina/happy-llm)

### å¼€æºåè®®

æœ¬é¡¹ç›®åŸºäº [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License](http://creativecommons.org/licenses/by-nc-sa/4.0/) åè®®å¼€æºã€‚

</div>

---

<div id="english">

### Project Introduction

After two hours spent learning Happy-LLM on Oct 14, 2025. Started this repo for clarity. It's a simpler, painless version for beginner. Reading it takes roughly 30-40 mins. I hope that will be helpful.

Updated PyTorch information on Oct 24, 2025.

## ğŸ“– Content Navigation

| Chapter                                          | Key Content                                                                          | Status |
| ------------------------------------------------ | ------------------------------------------------------------------------------------ | ------ |
| [Preface](./0.Pre.md)                            | Project origin, background, and reader recommendations                               | âœ…     |
| [Chapter 1: NLP](./1.NLP.md)                     | What is NLP, development history, task classification, text representation evolution | âœ…     |
| [Chapter 2: Transformer](./2.Transformer.md)     | Attention mechanism, Encoder-Decoder, hands-on Transformer building                  | âœ…     |
| [Chapter 3: PLM](./3.PLM.md)                     | Comparison of Encoder-only, Encoder-Decoder, Decoder-Only models                     | âœ…     |
| [Chapter 4: LLM](./4.LLM.md)                     | LLM definition, training strategies, emergent ability analysis                       | âœ…     |
| [Chapter 5: LLM Implement](./5.LLM_Implement.md) | Implementing LLaMA2, training Tokenizer, pre-training small LLM                      | âœ…     |
| [Chapter 6: Train](./6.Train.md)                 | Pre-training, supervised fine-tuning, LoRA/QLoRA efficient fine-tuning               | ğŸš§     |
| [Chapter 7: Usage](./7.Usage.md)                 | Model evaluation, RAG retrieval enhancement, Agent intelligent agents                | âœ…     |
| [PyTorch Basics](./PyTorch.md)                  | PyTorch training workflow, backpropagation, gradient descent, architecture comparison | âœ…     |

### Special Thanks

[Happy-LLM](https://github.com/datawhalechina/happy-llm)

### Open Source License

This work is licensed under a [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License](http://creativecommons.org/licenses/by-nc-sa/4.0/).

</div>
