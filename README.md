## Happy-LLM-Simple

---

[中文](#中文) | [English](#english)

---

<div id="中文">

### 项目简介

10 月 14 日晚上花了两个小时学习[Happy-LLM](https://github.com/datawhalechina/happy-llm)后，为了方便理解复盘开了这个仓库。相对原本更加明晰简短，可以理解为无痛小白版，整体读完大概三四十分钟左右。希望对大家有帮助。

10 月 24 日更新了 PyTorch 相关信息。

## 📖 目录导航

| 章节                                       | 主要内容                                             | 状态 |
| ------------------------------------------ | ---------------------------------------------------- | ---- |
| [前言](./0.Pre.md)                         | 项目起源、背景介绍和读者建议                         | ✅   |
| [第 1 章：NLP](./1.NLP.md)                 | 什么是 NLP、发展历程、任务分类、文本表示演进         | ✅   |
| [第 2 章：Transformer](./2.Transformer.md) | 注意力机制、编码器-解码器、动手构建 Transformer      | ✅   |
| [第 3 章：PLM](./3.PLM.md)                 | Encoder-only、Encoder-Decoder、Decoder-Only 模型对比 | ✅   |
| [第 4 章：LLM](./4.LLM.md)                 | LLM 定义、训练策略、涌现能力分析                     | ✅   |
| [第 5 章：LLM 实现](./5.LLM_Implement.md)  | 实现 LLaMA2、训练 Tokenizer、预训练小规模 LLM        | ✅   |
| [第 6 章：训练](./6.Train.md)              | 预训练、监督微调、LoRA/QLoRA 高效微调                | 🚧   |
| [第 7 章：应用](./7.Usage.md)              | 模型评估、RAG 检索增强、Agent 智能体                 | ✅   |
| [PyTorch 基础](./PyTorch.md)               | PyTorch 训练流程、反向传播、梯度下降、架构对比        | ✅   |

### 特别感谢

[Happy-LLM](https://github.com/datawhalechina/happy-llm)

### 开源协议

本项目基于 [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License](http://creativecommons.org/licenses/by-nc-sa/4.0/) 协议开源。

</div>

---

<div id="english">

### Project Introduction

After two hours spent learning Happy-LLM on Oct 14, 2025. Started this repo for clarity. It's a simpler, painless version for beginner. Reading it takes roughly 30-40 mins. I hope that will be helpful.

Updated PyTorch information on Oct 24, 2025.

## 📖 Content Navigation

| Chapter                                          | Key Content                                                                          | Status |
| ------------------------------------------------ | ------------------------------------------------------------------------------------ | ------ |
| [Preface](./0.Pre.md)                            | Project origin, background, and reader recommendations                               | ✅     |
| [Chapter 1: NLP](./1.NLP.md)                     | What is NLP, development history, task classification, text representation evolution | ✅     |
| [Chapter 2: Transformer](./2.Transformer.md)     | Attention mechanism, Encoder-Decoder, hands-on Transformer building                  | ✅     |
| [Chapter 3: PLM](./3.PLM.md)                     | Comparison of Encoder-only, Encoder-Decoder, Decoder-Only models                     | ✅     |
| [Chapter 4: LLM](./4.LLM.md)                     | LLM definition, training strategies, emergent ability analysis                       | ✅     |
| [Chapter 5: LLM Implement](./5.LLM_Implement.md) | Implementing LLaMA2, training Tokenizer, pre-training small LLM                      | ✅     |
| [Chapter 6: Train](./6.Train.md)                 | Pre-training, supervised fine-tuning, LoRA/QLoRA efficient fine-tuning               | 🚧     |
| [Chapter 7: Usage](./7.Usage.md)                 | Model evaluation, RAG retrieval enhancement, Agent intelligent agents                | ✅     |
| [PyTorch Basics](./PyTorch.md)                  | PyTorch training workflow, backpropagation, gradient descent, architecture comparison | ✅     |

### Special Thanks

[Happy-LLM](https://github.com/datawhalechina/happy-llm)

### Open Source License

This work is licensed under a [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License](http://creativecommons.org/licenses/by-nc-sa/4.0/).

</div>
